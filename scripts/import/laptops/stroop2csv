#!/usr/bin/env python

##
##  See COPYING file distributed along with the ncanda-data-integration package
##  for the copyright and license terms
##
from __future__ import print_function
from __future__ import division
from builtins import str
from builtins import range
from past.utils import old_div
import os
import re
import csv
import sys
import math
import argparse
import hashlib
import numpy as np
import pandas as pd
from pathlib import Path

from convert_util import post_issue


def post_issue_and_exit(issue_label, issue_title, **kwargs):
    import convert_util
    convert_util.post_issue_and_exit('stroop2csv', args.infile, args.verbose, args.post_to_github, issue_label, issue_title, **kwargs)

# Setup command line parser
parser = argparse.ArgumentParser( description="Convert e-Prime Stroop log file to CSV score file" )
parser.add_argument( "-v", "--verbose", help="Verbose operation", action="store_true")
parser.add_argument( "--mr-session", help="The input file is from an fMRI Stroop session, which uses numerical rather than letter key responses.", action="store_true")
parser.add_argument( "--record", help="Record ID (if not provided, this is generated from the information in the data file)", default=None )
parser.add_argument( "--event", help="Event in a longitudinal REDCap project (if not provided, a non-longitudinal project is assumed)", default=None )
parser.add_argument( "--overwrite", help="Overwrite existing CSV files.", action="store_true")
parser.add_argument("-p", "--post-to-github", help="Post all issues to GitHub instead of std out.", action="store_true",
                    default=False)
parser.add_argument("-t","--time-log-dir",
                    help="If set then time logs are written to that directory",
                    action="store",
                    default=None)
parser.add_argument( "infile", help="Input .txt Stroop file (as generated by e-Prime).")
parser.add_argument( "outdir", help="Output directory. All CSV files are created in this directory")
args = parser.parse_args()

# so files of type /fs/storage/laptops/ncanda/*/*/NCANDAStroopMtS_3cycles_7m53stask_100SD.txt are binary (file -b --mime-type <file> returns text/x-diff)
file = open( args.infile, 'rb').read().decode('utf-16').split("\r\n")
RT=[]
response=[]
procedure=[]
running=[]

subject = None
session = None

for s in file:
    if 'SessionDate:' in s:
        date_of_test = '%s-%s-%s' % (s[19:23], s[13:15], s[16:18])

    if 'SessionTime:' in s:
        time_of_day = '%s:%s' % (s[13:15], s[16:18])

    match = re.match( 'Subject:[^0-9]*([0-9]{1,5})', s )
    if match:
        subject = ("0000%s" % match.group(1))[-4:]
    match = re.match( 'Session:[^0-9]*([0-9]{1,5})', s )
    if match:
        session = ("0000%s" % match.group(1))[-4:]
    # get response time
    if (".RT" in s) and ("RTI" not in s):
        RT.append(s[s.find(":")+2:])
    # get response
    if (".RESP" in s) and ("RTI" not in s):
        response.append(s[s.find(":")+2:])
    # get procedures
    if "Procedure:" in s and "Block" not in s:
        procedure.append(s[s.find(":")+2:])
    # get runnings
    if "Running:" in s and "ListRun" not in s:
        running.append(s[s.find(":")+2:])

if not subject or not session:
    errMsg = "ERROR: no subject or session ID in Stroop file %s" % args.infile 
    post_issue_and_exit(hashlib.sha1('stroop2csv {}'.format(errMsg).encode()).hexdigest()[0:6], errMsg)
    # should be done differently but if called without -p option it is not posted when script is run from harvester

digit_to_site = { '1': 'A', '2': 'B', '3': 'C', '4': 'D', '5': 'E', '6': 'F', '7': 'G', '8': 'H', '9': 'I', '0': 'J' }
digit_to_sex = { '1': 'M', '2': 'F', '3': 'X', '4': 'X', '5': 'X', '6': 'X', '7': 'X', '8': 'X', '9': 'T', '0': 'X' }

subject_id = '%s-%s%s-%s-%s' % ( digit_to_site[subject[0]], subject[1:4], session[0:2], digit_to_sex[session[2]], session[3] )

if args.record:
    record_id = args.record
else:
    record_id = '%s-%s' % ( subject_id, date_of_test )

output_filename = os.path.join(args.outdir, '%s.csv' % record_id)
if os.path.exists(output_filename):
    if args.overwrite:
        pass
    else:
        if args.verbose : 
            errMsg = "Warning: output file {} already exists. Scores are not updated!".format(output_filename)
            post_issue_and_exit(hashlib.sha1('stroop2csv {}'.format(errMsg).encode()).hexdigest()[0:6], errMsg)
        else: 
            errMsg = 0

        sys.exit(errMsg)

# Check that the extracted ID hasn't changed from sites
_upload_path = Path(args.infile).parent.name
_upload_id = _upload_path[0:11]
_upload_site = _upload_path[0]
if subject_id[0] != _upload_site and (_upload_site in 'ABCDE'):
    post_issue(script='stroop2csv',
               infile=args.infile,
               verbose=args.verbose,
               post_to_github=args.post_to_github,
               issue_label="StroopIDChange",
               issue_title="Warning: stroop2csv file changed sites",
               to_resolve="Instruct the site to review the Import record "
                          "and, if appropriate, correct the subject ID.",
               post_resolution_instructions="Close after notifying site.",
               reopened="If this issue has reopened, the file is being "
               "reprocessed. Add it to special_cases.yml::harvester::ignore.",
               old_id=_upload_id,
               new_id=subject_id)

# convert response time to integer
RT=[float(x) for x in RT]

# matching = [procedure for i, procedure in procedure if test_ref[1] in procedure]
# print(matching)


# this reference help to find correct answers
test_ref=["ConMRR","ConM",'ConMNMRS','ConM','ConNMRR','ConNM','ConMNMRS','ConNM','IncMRR','IncM','IncMNMRS','IncM','IncNMRR','IncNM','IncMNMRS','IncNM']

if args.mr_session:
    correct_ans=['1','1','2','2','1','1','2','2']
else:
    correct_ans=['b','b','n','n','b','b','n','n']

mean=['NA','NA','NA','NA','NA','NA','NA','NA','NA']
std=['NA','NA','NA','NA','NA','NA','NA','NA','NA']
median=['NA','NA','NA','NA','NA','NA','NA','NA','NA']
z=['NA','NA','NA','NA','NA','NA','NA','NA','NA']
error=['0','0','0','0','0','0','0','0','0']
#prolong=['0','0','0','0','0','0','0','0','0']
miss=['0','0','0','0','0','0','0','0','0']
Diff=['NA','NA','NA','NA','NA','NA','NA','NA']
zs=['NA','NA','NA','NA','NA','NA','NA','NA']
total_correct_time=[]
response_error=[]

# this is to get items meet certain criteria
def get_item(criteria,all_record):
    value = [i for i in range(len(all_record)) if criteria in all_record[i]]
    return value

def samplestd(data):
    samplestd = np.nan
    try:
        mean=data.mean()
        if len(data) > 1 :
            samplestd=math.sqrt(old_div(sum((data-mean)**2),(len(data)-1)))

    except RuntimeWarning as e:
        post_issue_and_exit(hashlib.sha1('stroop2csv {}'.format(e).encode()).hexdigest()[0:6], "Error: could not compute samplestd", 
                  err_msg = str(e),
                  data_contained = data)

    return samplestd



for j in range(1,9):
    index1 = get_item(test_ref[2*j-2],running)
    index2 = get_item(test_ref[2*j-1],procedure)
    # index2 = [i for i in range(len(procedure)) if  in procedure[i]]
    # index is the test for 8 different categories
    index=set(index1).intersection(set(index2))
    # print(len(index))
    # get Response and Time
    response_subset= [response[i] for i in index]
    response_error_subset= [response[i] for i in index]
    RT_subset= [RT[i] for i in index]

    #get rid of too long or too short response
    index_range = [i for i in range(len(index)) if (RT_subset[i]<= 2300 and RT_subset[i]>150)]
    RT_subset = [RT_subset[i] for i in index_range]
    response_subset = [response_subset[i] for i in index_range]
    response_error_subset = [response_error_subset[i] for i in index_range]
    #running = [running[i] for i in index_range]
    #procedure = [procedure[i] for i in index_range]
    miss[j-1] = len(index)-len(index_range)

    # get correct responses
    index_correct=get_item(correct_ans[j-1],response_subset)
    error[j-1]=len(response_subset)-len(index_correct)
    # get all the correct response time and change them to array
    time_correct=[RT_subset[i] for i in index_correct]
    time_correct=np.asarray(time_correct)
    for i in index_correct:
        response_error_subset[i]=u''

    # calculate mean and std from all the correct response
    if len( time_correct ):
        mean1=time_correct.mean()
        if len( time_correct ) > 1:
            std1 = samplestd(time_correct)
        else:
            std1 = 0
    else:
        mean1 = np.nan
        std1 = np.nan

    # get rid of prolonged response, that is 3*std above the mean
    #noprolong_index=[i for i in range(len(time_correct)) if time_correct[i]<= (mean1+3*std1)]
    # final correct/no_prolonged response time
    #final_time_list=[time_correct[i] for i in noprolong_index]
    #final_time=np.asarray(final_time_list)
    #prolong[j-1]=len(time_correct)-len(noprolong_index)

    # calculate mean, std, z scores
    if len( time_correct ):
        mean[j-1]=time_correct.mean()
        std[j-1]= samplestd(time_correct)
        median[j-1]= np.median(time_correct)
    else:
        mean[j-1] = np.nan
        std[j-1] = np.nan
        median[j-1] = np.nan

    # collect all the correct response time
    total_correct_time.extend(time_correct)
    response_error.extend(response_error_subset)

total_correct_time=np.asarray(total_correct_time)
if len( total_correct_time ):
    mean[8]=total_correct_time.mean()
    std[8]=samplestd(total_correct_time)
    median[8]=np.median(total_correct_time)
else:
    mean[8] = np.nan
    std[8] = np.nan
    median[8] = np.nan
# mean-3*std
z[8]=mean[8]-3*std[8]
# mean+3*std
error[8]=mean[8]+3*std[8]

# calculate z scores
for j in range(1,9):
    if std[j-1] > 0:
        z[j-1]=old_div((mean[j-1]-mean[8]),std[j-1])
    else:
        z[j-1] = np.nan

Diff[0]=mean[4]-mean[0]
Diff[1]=mean[6]-mean[2]
Diff[2]=mean[5]-mean[1]
Diff[3]=mean[7]-mean[3]
Diff[4]=0.5*(Diff[0]+Diff[2])
Diff[5]=0.5*(Diff[1]+Diff[3])
Diff[6]=0.5*(Diff[0]+Diff[2])
Diff[7]=0.5*(Diff[2]+Diff[3])
zs[0]=z[4]-z[0]
zs[1]=z[6]-z[2]
zs[2]=z[5]-z[1]
zs[3]=z[7]-z[3]
zs[4]=0.5*(z[0]+z[2])
zs[5]=0.5*(z[1]+z[3])
zs[6]=0.5*(z[0]+z[2])
zs[7]=0.5*(z[2]+z[3])

if not os.path.exists(args.outdir):
    os.makedirs(args.outdir)

if args.mr_session:
    record_id_variable = "study_id"
else:
    record_id_variable = "record_id"

title=[ record_id_variable,"stroop_complete",
       "stroop_total_mean","stroop_total_std", "stroop_total_median", "stroop_mean_3stdl","stroop_mean_3stdu", "stroop_conm_rr_mean","stroop_conm_rs_mean","stroop_connm_rr_mean","stroop_connm_rs_mean", "stroop_incm_rr_mean","stroop_incm_rs_mean",
       "stroop_incnm_rr_mean","stroop_incnm_rs_mean","stroop_conm_rr_std","stroop_conm_rs_std", "stroop_connm_rr_std","stroop_connm_rs_std","stroop_incm_rr_std","stroop_incm_rs_std","stroop_incnm_rr_std","stroop_incnm_rs_std",
 "stroop_conm_rr_median","stroop_conm_rs_median", "stroop_connm_rr_median","stroop_connm_rs_median","stroop_incm_rr_median","stroop_incm_rs_median","stroop_incnm_rr_median","stroop_incnm_rs_median",
       "stroop_conm_rr_z","stroop_conm_rs_z","stroop_connm_rr_z","stroop_connm_rs_z","stroop_incm_rr_z","stroop_incm_rs_z","stroop_incnm_rr_z", "stroop_incnm_rs_z","stroop_conm_rr_error","stroop_conm_rs_error",
       "stroop_connm_rr_error","stroop_connm_rs_error","stroop_incm_rr_error", "stroop_incm_rs_error","stroop_incnm_rr_error","stroop_incnm_rs_error","stroop_conm_rr_miss","stroop_conm_rs_miss",
       "stroop_connm_rr_miss","stroop_connm_rs_miss","stroop_incm_rr_miss", "stroop_incm_rs_miss","stroop_incnm_rr_miss","stroop_incnm_rs_miss", "stroop_stroopm_rr_diffrt","stroop_stroopnm_rr_diffrt","stroop_stroopm_rs_diffrt",  "stroop_stroopnm_rs_diffrt","stroop_stroopm_diffrt","stroop_stroopnm_diffrt","stroop_stroop_rr_diffrt","stroop_stroop_rs_diffrt","stroop_stroopm_rr_z","stroop_stroopnm_rr_z", "stroop_stroopm_rs_z","stroop_stroopnm_rs_z",
       "stroop_stroopm_z","stroop_stroopnm_z","stroop_rr_z","stroop_rs_z"]

result=[record_id,1,mean[8],std[8],median[8],z[8],error[8],
        mean[0],mean[1],mean[2],mean[3],mean[4],mean[5],mean[6],mean[7],std[0],std[1],std[2],std[3],std[4],std[5],
        std[6],std[7],median[0],median[1],median[2],median[3],median[4],median[5],median[6],median[7],z[0],z[1],z[2],
        z[3],z[4],z[5],z[6],z[7],error[0],error[1],error[2],error[3],error[4],error[5], error[6],error[7],
        miss[0],miss[1],miss[2],miss[3],miss[4],miss[5], miss[6],miss[7],Diff[0],
        Diff[1],Diff[2],Diff[3],Diff[4],Diff[5],Diff[6],Diff[7],zs[0],zs[1],zs[2],zs[3],zs[4],zs[5],zs[6],zs[7]]


title.append( 'stroop_timeofday' )
result.append( time_of_day )

if args.mr_session:
    # If this is for the MRI Stroop, add date also and prefix all fields accordingly
    title.append( 'stroop_date' )
    result.append( date_of_test )
    title = [ re.sub ( '^stroop_', 'mri_stroop_', t ) for t in title ]
else:
    # Non-MRI session - add import-specific fields
    title += [ "visit_information_complete" ]
    result += [ '1' ]

# If an event for the REDCap project is given, add to list of field
if args.event:
    title.append( 'redcap_event_name' )
    result.append( args.event )

result_dict = dict(zip(title, result))
# Drop useless z-scores which are apparently non-representative
useless_cols = [
    "stroop_conm_rr_z",
    "stroop_conm_rs_z",
    "stroop_connm_rr_z",
    "stroop_connm_rs_z",
    "stroop_incm_rr_z",
    "stroop_incm_rs_z",
    "stroop_incnm_rr_z",
    "stroop_incnm_rs_z",
    "stroop_stroopm_rr_z",
    "stroop_stroopnm_rr_z",
    "stroop_stroopm_rs_z",
    "stroop_stroopnm_rs_z",
    "stroop_stroopm_z",
    "stroop_stroopnm_z",
    "stroop_rr_z",
    "stroop_rs_z"
]
result_dict = {key: [val] for key, val in result_dict.items()
               if key not in useless_cols}
result_df = pd.DataFrame(result_dict)
result_df.to_csv(output_filename, index=False, na_rep='',
                 quoting=csv.QUOTE_ALL)

print(output_filename)

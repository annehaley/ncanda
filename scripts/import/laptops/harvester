#!/usr/bin/env python

##
##  See COPYING file distributed along with the ncanda-data-integration package
##  for the copyright and license terms
##
import re
import os
import argparse
import subprocess

import sys

from sibisBeta import sibislogger as slog
import hashlib

# Setup command line parser
parser = argparse.ArgumentParser(description="Harvest incoming data files from "
                                             "SVN repository, call correct converter "
                                             "programs, and upload generated CSV files "
                                             "to REDCap")
parser.add_argument("-v", "--verbose",
                    help="Verbose operation",
                    action="store_true")
parser.add_argument("--include-testing",
                    help="Include 'testing' data (as marked by a 'T' instead of a gender"
                         " code in the subject ID). Currently for single-session files "
                         "only.",
                    action="store_true")
parser.add_argument("--overwrite",
                    help="Overwrite existing CSV files.",
                    action="store_true")
parser.add_argument("--force-upload",
                    help="Force upload of records to REDCap even if a record of the "
                         "same name already exists.",
                    action="store_true")
parser.add_argument("--file-to-upload",
                    help="Absolute file path for path to CSV file.")
parser.add_argument("svndir",
                    help="Input directory. This is the Subversion-controlled working"
                         " directory checked out from the laptop data submission repository.")
parser.add_argument("outdir",
                    help="Output directory. All CSV files are created in subdirectories"
                         " of this directory.")
parser.add_argument("-p", "--post-to-github", help="Post all issues to GitHub instead of std out.", action="store_true",
                    default=False)
parser.add_argument("-t","--time-log-dir",
                    help="If set then time logs are written to that directory (e.g. /fs/ncanda-share/ncanda-data-log/crond)",
                    action="store",
                    default=None)
args = parser.parse_args()

# Setup logging
slog.init_log(args.verbose, args.post_to_github,'NCANDA XNAT: Harvester Message', 'harvester')
slog.startTimer1()

# Figure out where this script is, so we can get path to other scripts.

bindir = os.path.dirname(os.path.realpath(__file__))


def run_converter(site, command):
    """
    Conversion tool.

    :param site: str
    :param command: str
    :return:
    """
    if args.verbose:
        print "Running", ' '.join(command)
    try:
        added_files = subprocess.check_output(command)
        if len(added_files):
            for fi in added_files.strip().split('\n'):
                if re.match('.*\.csv$', fi):
                    try:
                        if args.verbose:
                            print "Importing", fi, "into REDCap"
                        if args.force_upload:
                            subprocess.call([os.path.join(bindir, 'csv2redcap'), '--force-update', args.post_to_github,
                                             '--data-access-group', site, fi])
                        else:
                            subprocess.call([os.path.join(bindir, 'csv2redcap'), args.post_to_github,
                                             '--data-access-group', site, fi])
                    except:
                        error = "Failed importing files into REDCap"
                        slog.info(hashlib.sha1('harvester {}'.format(fi)).hexdigest()[0:6], error,
                                  file=str(fi))
                else:
                    print 'NOT A CSV FILE:', fi
    except:
        pass
#
# Function: hand file to correct converter
#
def handle_file( path, site, filename):
    # Prepare option for overwriting
    if args.overwrite:
        overwrite = ["--overwrite"]
    else:
        overwrite = []

    if args.post_to_github:
        post_to_github = ["--post-to-github"]
    else:
        post_to_github = []

    # Is this a LimeSurvey file?
    if re.match( '^survey.*\.csv$', filename ):
        run_converter( site, [ os.path.join( bindir, "lime2csv" ) ] + overwrite + post_to_github + [ path, os.path.join( args.outdir, site, "limesurvey" ) ] )
    # Is this a Stroop file (Note: the "_100SD-" is signifigant as some MRI
    # Stroop files will include "_100SDMirror" in the filename)?
    elif re.match('^NCANDAStroopMtS_3cycles_7m53stask_100SD-[^/]*\.txt$', filename):
        run_converter(site, [os.path.join( bindir, "stroop2csv")] + overwrite + [path, os.path.join(args.outdir, site, "stroop")])
        try:
            subprocess.check_output([os.path.join(bindir, "eprime2redcap"), path, 'stroop_log_file'])
        except:
            slog.info(hashlib.sha1('harvester').hexdigest()[0:6],
                          "ERROR: could not upload Stroop file",
                          subject_label=path.split('/')[-2],
                          filename=filename,
                          script='harvester')
    # Is this a Delayed Discounting file?
    elif re.match( '.*V12\.txt$', filename ):
        run_converter( site, [ os.path.join( bindir, "dd2csv" ) ] + overwrite + post_to_github + [ path, os.path.join( args.outdir, site, "deldisc" ) ] )
    # Is this a PASAT (Access) database?
    elif re.match( '^PASAT_Stnd.*\.mdb$', filename ):
        run_converter( site, [ os.path.join( bindir, "pasat2csv" ) ] + overwrite + [ path, os.path.join( args.outdir, site, "pasat" ) ] )
    # Is this a SSAGA (Blaise) database?
    elif re.match( '^NSSAGA_v3\.bdb$', filename ) or re.match( '.*\.[Aa][Ss][Cc]$', filename ):
        if 'Youth_SAAGAv3' in path:
            run_converter( site, [ os.path.join( bindir, "blaise2csv" ) ] + overwrite + [ path, 'youth', os.path.join( args.outdir, site, "ssaga" ) ] )
        elif 'Parent_SAAGAv3' in path:
            run_converter( site, [ os.path.join( bindir, "blaise2csv" ) ] + overwrite + [ path, 'parent', os.path.join( args.outdir, site, "ssaga" ) ] )
        else:
            print "ERROR: could not determine whether",path,"contains Youth or Parent SSAGA"

#
# Function: handle updated file by dispatching to the correct subhandler
#
def handle_file_update( path ):
    # First, let's get the site ID from the path
    match_site = re.search( 'ncanda/([A-Za-z]*)[^/]*/(.*)', path )

    if match_site:
        # Get the site ID
        site = match_site.group( 1 )
        # We do not accept data from the "admin" machines - testing only, not a collection site
        if site == 'admin':
            return

        filename = re.search( '(.*)/([^/].*)', path ).group( 2 );
        handle_file( path, site, filename )
                            
#
# Callback: catch files added and updated since last svn update
#
updated_files = []
import pysvn
def notify( event_dict ):
    global updated_files
    if event_dict['kind'] == pysvn.node_kind.file:
        if event_dict['action'] == pysvn.wc_notify_action.update_add or event_dict['action'] == pysvn.wc_notify_action.update_update or event_dict['action'] == pysvn.wc_notify_action.restore:
            updated_files.append( event_dict['path'] )

# Main function: perform svn update and catch all resulting events
try:
    client = pysvn.Client()
    client.callback_notify = notify
    client.update( args.svndir )
except pysvn._pysvn.ClientError as e:
    slog.info(hashlib.sha1('harvester').hexdigest()[0:6],
                  "ERROR: Failed to create a Client for pysvn",
                  script='harvester',
                  msg=str(e))
    sys.exit(1)

# Append single file to upload
if args.file_to_upload:
    updated_files.append(args.file_to_upload)

# Process all updated or added files
for file in updated_files:
    handle_file_update( file )

slog.takeTimer1("script_time","{'records': " + str(len(updated_files)) + "}")


#!/usr/bin/env python

##
##  See COPYING file distributed along with the ncanda-data-integration package
##  for the copyright and license terms
##
import re
import os
import argparse
import subprocess

import sys

from sibispy import sibislogger as slog
from sibispy import utils as sutils
import sibispy 

import hashlib
updated_files = []
import pysvn

# Setup command line parser
parser = argparse.ArgumentParser(description="Harvest incoming data files from "
                                             "SVN repository, call correct converter "
                                             "programs, and upload generated CSV files "
                                             "to REDCap")
parser.add_argument("-v", "--verbose",
                    help="Verbose operation",
                    action="store_true")
parser.add_argument("--include-testing",
                    help="Include 'testing' data (as marked by a 'T' instead of a gender"
                         " code in the subject ID). Currently for single-session files "
                         "only.",
                    action="store_true")
parser.add_argument("--overwrite",
                    help="Overwrite existing CSV files.",
                    action="store_true")
parser.add_argument("--force-upload",
                    help="Force upload of records to REDCap even if a record of the "
                         "same name already exists.",
                    action="store_true")
parser.add_argument("--file-to-upload",
                    help="Absolute file path for path to CSV file.")
parser.add_argument("-p", "--post-to-github", help="Post all issues to GitHub instead of std out.", action="store_true",
                    default=False)
parser.add_argument("-t","--time-log-dir",
                    help="If set then time logs are written to that directory",
                    action="store",
                    default=None)
parser.add_argument("--date",
                    help="If set then uploads all entries during that time period, e.g. {2017-06-29}:{2017-07-03} uploads everything that was in the svn repository from 2017-06-29 to 2017-07-03",
                    action="store",
                    default=None)

args = parser.parse_args()

# Setup logging
slog.init_log(args.verbose, args.post_to_github,'NCANDA Import-Laptop: harvester message', 'harvester', args.time_log_dir)
slog.startTimer1()

session = sibispy.Session()
if not session.configure() :
    sys.exit()

if not session.connect_server('svn_laptop', True) : 
    sys.exit()

svndir = os.path.join(session.get_laptop_dir(),'ncanda')
outdir = os.path.join(session.get_laptop_dir(),'imported')

# Figure out where this script is, so we can get path to other scripts.

bindir = os.path.dirname(os.path.realpath(__file__))


def run_converter(site, command,verbose):
    """
    Conversion tool.

    :param site: str
    :param command: str
    :return:
    """
    if verbose:
        print "Running", ' '.join(command)
    try:
        added_files = subprocess.check_output(command)
        if not len(added_files):
            if verbose : 
                print "No files were transferred to csv" 
            return 


        for fi in added_files.strip().split('\n'):
            if re.match('.*\.csv$', fi):
                try:
                    if verbose:
                        print "Importing", fi, "into REDCap"

                    command_array = [os.path.join(bindir, 'csv2redcap')]
                    if args.force_upload:
                        command_array += ['--force-update']
                    if args.post_to_github:
                        command_array += ["-p"]
                    if args.time_log_dir:
                        command_array += ["-t", args.time_log_dir]

                    command_array += ['--data-access-group', site, fi]
                        
                    subprocess.call(command_array)

                except:
                    error = "Failed importing files into REDCap"
                    slog.info(hashlib.sha1('harvester {}'.format(fi)).hexdigest()[0:6], error,
                              file=str(fi),
                              command=str(command_array))
            else:
                slog.info("harvester-" + hashlib.sha1(str(fi)).hexdigest()[0:6], 'NOT A CSV FILE',
                          file=str(fi),
                          command=str(command_array))

    except Exception as emsg:
        cmd = ' '.join(command)
        slog.info('harvester-' + hashlib.sha1(str(emsg)).hexdigest()[0:6], 'Failed to transform file into csv',
                  cmd=str(cmd),
                  err_msg = str(emsg))
#
# Function: hand file to correct converter
#
def handle_file( path, site, filename, verbose):
    # Prepare option for overwriting
    if args.overwrite:
        overwrite = ["--overwrite"]
    else:
        overwrite = []

    if args.post_to_github:
        post_to_github = ["--post-to-github"]
    else:
        post_to_github = []

    subject_label = path.split('/')[-2]
    # Is this a LimeSurvey file?
    if re.match( '^survey.*\.csv$', filename ):
        run_converter( site, [ os.path.join( bindir, "lime2csv" ) ] + overwrite + post_to_github + [ path, os.path.join(outdir, site, "limesurvey" ) ], verbose )
    # Is this a Stroop file (Note: the "_100SD-" is signifigant as some MRI
    # Stroop files will include "_100SDMirror" in the filename)?
    elif re.match('^NCANDAStroopMtS_3cycles_7m53stask_100SD-[^/]*\.txt$', filename):
        run_converter(site, [os.path.join( bindir, "stroop2csv")] + overwrite + post_to_github + [path, os.path.join(outdir, site, "stroop")], verbose)
        try:
            subprocess.check_output([os.path.join(bindir, "eprime2redcap"), path, 'stroop_log_file'])
        except:
            slog.info(subject_label,
                          "ERROR: could not upload Stroop file",
                          filename=filename,
                          script='harvester')
    # Is this a Delayed Discounting file?
    # ignoes the V12-All.txt file 
    elif re.match( '.*V12\.txt$', filename ):
        run_converter( site, [ os.path.join( bindir, "dd2csv" ) ] + overwrite + post_to_github + [ path, os.path.join( outdir, site, "deldisc" ) ], verbose )
    # Is this a PASAT (Access) database?
    elif re.match( '^PASAT_Stnd.*\.mdb$', filename ):
        run_converter( site, [ os.path.join( bindir, "pasat2csv" ) ] + overwrite + [ path, os.path.join( outdir, site, "pasat" ) ], verbose )
    # Is this a SSAGA (Blaise) database?
    elif re.match( '^NSSAGA_v3\.bdb$', filename ) or re.match( '.*\.[Aa][Ss][Cc]$', filename ):
        command_array = [os.path.join(bindir, 'wine/blaise2csv')]
        if args.time_log_dir:
            command_array += ["-t", args.time_log_dir]
        if args.post_to_github:
            command_array += ["-p"]
        if 'Youth_SAAGAv3' in path:
            command_array += overwrite + [ path, 'youth', os.path.join( outdir, site, "ssaga" ) ]
        elif 'Parent_SAAGAv3' in path:
            command_array += overwrite + [ path, 'parent', os.path.join( outdir, site, "ssaga" ) ]
        else:
            slog.info(subject_label, 'ERROR: could not determine whether the path contains Youth or Parent SSAGA',
                      path=str(path))
        run_converter(site,command_array, verbose)
    elif verbose : 
        print "Warning: No conversion for file found!"

#
# Function: handle updated file by dispatching to the correct subhandler
#
def handle_file_update( path, verbose ):
    # First, let's get the site ID from the path
    match_site = re.search( 'ncanda/([A-Za-z]*)[^/]*/(.*)', path )

    if match_site:
        # Get the site ID
        site = match_site.group( 1 )
        # We do not accept data from the "admin" machines - testing only, not a collection site
        if site == 'admin':
            return

        filename = re.search( '(.*)/([^/].*)', path ).group( 2 );
        handle_file( path, site, filename, verbose)
                            
#
# Callback: catch files added and updated since last svn update
#
def notify( event_dict ):
    global updated_files
    if event_dict['kind'] == pysvn.node_kind.file:
        if event_dict['action'] == pysvn.wc_notify_action.update_add or event_dict['action'] == pysvn.wc_notify_action.update_update or event_dict['action'] == pysvn.wc_notify_action.restore:
            updated_files.append( event_dict['path'] )

#
# Main function: perform svn update and catch all resulting events
#
if args.verbose :
    print "Run svn update ..." 

if args.date : 
   cmd ='cd {}; svn diff --no-auth-cache --username {} --password \'{}\' --summarize -r {} | grep -v "D     "  | tr " " "," | cut -d, -f8 | grep "\." | tr "\n" ","'.format( svndir, session.api['svn_laptop']['user'], session.api['svn_laptop']['password'], args.date )
   process = subprocess.Popen(cmd,shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE)
   out_std,out_err = process.communicate()
   if out_err :
       slog.info(hashlib.sha1('harvester' + out_err).hexdigest()[0:6],
                 "ERROR: Failed to update files in range " + args.date + ". Continued without them!",
                 script='harvester',
                 msg=out_err)
   else :
       if out_std :
           for fName in out_std[:-1].split(',') :
               updated_files.append(os.path.join(svndir,fName))               
        
elif args.file_to_upload:
    updated_files.append(args.file_to_upload)

else : 
    if not session.run_svn('update', callbackNotifyFct = notify) : 
       sys.exit(1)

if args.verbose :
    print "... done " 
    print "Files to be uploaded :\n" + "\n".join(updated_files)
    print "Number of records uploading: ", len(updated_files)
 
# Append single file to upload

# Process all updated or added files
for file in updated_files:
    handle_file_update( file , args.verbose)

slog.takeTimer1("script_time","{'records': " + str(len(updated_files)) + "}")



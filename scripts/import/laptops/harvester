#!/usr/bin/env python

##
##  See COPYING file distributed along with the ncanda-data-integration package
##  for the copyright and license terms
##
import re
import os
import argparse
import subprocess

import sys

from sibispy import sibislogger as slog
import sibispy 

import hashlib
updated_files = []
import pysvn
import datetime 
# Setup command line parser
parser = argparse.ArgumentParser(description="Harvest incoming data files from "
                                             "SVN repository, call correct converter "
                                             "programs, and upload generated CSV files "
                                             "to REDCap")
parser.add_argument("-v", "--verbose",
                    help="Verbose operation",
                    action="store_true")
parser.add_argument("--include-testing",
                    help="Include 'testing' data (as marked by a 'T' instead of a gender"
                         " code in the subject ID). Currently for single-session files "
                         "only.",
                    action="store_true")
parser.add_argument("--overwrite",
                    help="Overwrite existing CSV files.",
                    action="store_true")
parser.add_argument("--force-upload",
                    help="Force upload of records to REDCap even if a record of the "
                         "same name already exists.",
                    action="store_true")
parser.add_argument("--file-to-upload",
                    help="Absolute file path for path to CSV file.")
parser.add_argument("-p", "--post-to-github", help="Post all issues to GitHub instead of std out.", action="store_true",
                    default=False)
parser.add_argument("--only-converter-post-to-github", help="Do not double post issues to GitHub that are produced by commands called during conversion..", action="store_true", default=False)
parser.add_argument("-t","--time-log-dir",
                    help="If set then time logs are written to that directory",
                    action="store",
                    default=None)
parser.add_argument("--date",
                    help="If set then uploads all entries during that time period, e.g. {2017-06-29}:{2017-07-03} uploads everything that was in the svn repository from 2017-06-29 to 2017-07-03",
                    action="store",
                    default=None)

parser.add_argument("--last-week",
                    help="If set then uploads all entries during the last week", 
                    action="store_true",
                    default=False)

parser.add_argument("--all",
                    help="If set then uploads all entries", 
                    action="store_true",
                    default=False)


parser.add_argument("--no-blaise",
                    help="Blaise files are going to be ignored", 
                    action="store_true",
                    default=False)

parser.add_argument("--stroop-only",
                    help="Only upload stroop file - should only be done in combination with --date as otherwise other records are not uploaded in the future (unless --date is used without the --stroop-only)", 
                    action="store_true",
                    default=False)

args = parser.parse_args()

# Setup logging
slog.init_log(args.verbose, args.post_to_github,'NCANDA Import-Laptop: harvester message', 'harvester', args.time_log_dir)
slog.startTimer1()

session = sibispy.Session()
if not session.configure() :
    sys.exit()

if not session.connect_server('svn_laptop', True) : 
    sys.exit()

svndir = os.path.join(session.get_laptop_svn_dir())
outdir = os.path.join(session.get_laptop_imported_dir())

# Figure out where this script is, so we can get path to other scripts.

bindir = os.path.dirname(os.path.realpath(__file__))


def run_converter(site, subject_label, command,verbose):
    """
    Conversion tool.

    :param site: str
    :param command: str
    :return:
    """
    if verbose:
        print "Running", ' '.join(command)

    filesProcessed = 0 
    try : 
        added_files = subprocess.check_output(command)
        if not len(added_files):
            if verbose : 
                print "No files were transferred to csv" 
            return filesProcessed 


        for fi in added_files.strip().split('\n'):
            if re.match('.*\.csv$', fi):
                if os.path.basename(fi).split('-')[0] == 'NOID' : 
                    slog.info(subject_label + "-" +  hashlib.sha1('harvester {}'.format(fi)).hexdigest()[0:6], 'converter_cmd created file that does not contain subject ID and thus cannot be uploaded to redcap',
                              file=str(fi),
                              converter_cmd=" ".join(command), 
                              harvester_cmd=" ".join(sys.argv)) 
                else : 
                    try:
                        if verbose:
                            print "Importing", fi, "into REDCap"

                        command_array = [os.path.join(bindir, 'csv2redcap')]
                        if args.force_upload:
                            command_array += ['--force-update']
                        if args.post_to_github:
                            command_array += ["-p"]
                        if args.time_log_dir:
                            command_array += ["-t", args.time_log_dir]

                        command_array += ['--data-access-group', site, fi]
                        
                        if verbose: 
                            print "Running: " + ' '.join(command_array)

                        subprocess.call(command_array)
                        filesProcessed += 1

                    except:
                        error = "Failed importing files into REDCap"
                        slog.info(subject_label + "-" +  hashlib.sha1('harvester {}'.format(fi)).hexdigest()[0:6], error,
                                  file=str(fi),
                                  converter_cmd=" ".join(command), 
                                  harvester_cmd=" ".join(sys.argv)) 
            else:
                slog.info(subject_label + "-" +  hashlib.sha1('harvester {}'.format(fi)).hexdigest()[0:6], 'NOT A CSV FILE',
                          file=str(fi),
                          converter_cmd=" ".join(command), 
                          harvester_cmd=" ".join(sys.argv)) 

    except Exception as emsg:
        if args.only_converter_post_to_github:
            slog.sibisLogging().info(subject_label + "-" + hashlib.sha1(str(emsg)).hexdigest()[0:6], 'Failed to convert data into redcap conform csv file',
                  converter_cmd=" ".join(command), 
                  harvester_cmd=" ".join(sys.argv), 
                  err_msg = str(emsg))
        else :
            slog.info(subject_label + "-" + hashlib.sha1(str(emsg)).hexdigest()[0:6], 'Failed to convert data into redcap conform csv file',
                  converter_cmd=" ".join(command), 
                  harvester_cmd=" ".join(sys.argv), 
                  err_msg = str(emsg))

    return filesProcessed
#
# Function: hand file to correct converter
#
def handle_file( path, site, filename, verbose):
    # Prepare option for overwriting
    if args.overwrite:
        overwrite = ["--overwrite"]
    else:
        overwrite = []

    if args.post_to_github or args.only_converter_post_to_github:
        post_to_github = ["--post-to-github"]
    else:
        post_to_github = []

    subject_label = path.split('/')[-2]
    # Is this a LimeSurvey file?
    if re.match( '^survey.*\.csv$', filename ):
        # Never post to github as ou otherwise get two error messages - one from harvester and one from lime2csv 
        run_converter( site, subject_label, [ os.path.join( bindir, "lime2csv" ) ] + overwrite + post_to_github + [ path, os.path.join(outdir, site, "limesurvey" ) ], verbose )
    # Is this a Stroop file (Note: the "_100SD-" is signifigant as some MRI
    # Stroop files will include "_100SDMirror" in the filename)?
    elif re.match('^NCANDAStroopMtS_3cycles_7m53stask_100SD-[^/]*\.txt$', filename):
        filesProcessed = run_converter(site, subject_label, [os.path.join( bindir, "stroop2csv")] + overwrite +  post_to_github + [path, os.path.join(outdir, site, "stroop")], verbose)
        # Only perform eprime2 redcap if anything was processed 
        if filesProcessed > 0 : 
            eprime_cmd = [os.path.join(bindir, "eprime2redcap"), path, 'stroop_log_file']
            try:
                subprocess.check_output(eprime_cmd)
            except:
                slog.info(subject_label, 
                          "ERROR: could not upload Stroop file",
                          filename=filename,
                          converter_cmd=" ".join(eprime_cmd), 
                          harvester_cmd=" ".join(sys.argv))
    elif args.stroop_only : 
        return
    # Is this a Delayed Discounting file?
    # ignoes the V12-All.txt file 
    elif re.match( '.*V12\.txt$', filename ):
        run_converter( site, subject_label, [ os.path.join( bindir, "dd2csv" ) ] + overwrite + post_to_github + [ path, os.path.join( outdir, site, "deldisc" ) ], verbose )
    # Is this a PASAT (Access) database?
    elif re.match( '^PASAT_Stnd.*\.mdb$', filename ):
        run_converter( site, subject_label, [ os.path.join( bindir, "pasat2csv" ) ] + overwrite + [ path, os.path.join( outdir, site, "pasat" ) ], verbose )
    # Is this a SSAGA (Blaise) database?
    elif re.match( '^NSSAGA_v3\.bdb$', filename ) or re.match( '.*\.[Aa][Ss][Cc]$', filename ):
        if args.no_blaise :
            return 

        command_array = [os.path.join(bindir, 'wine/blaise2csv')] + overwrite + post_to_github
        if args.time_log_dir:
            command_array += ["-t", args.time_log_dir]
        if 'Youth_SAAGAv3' in path:
            command_array += [ path, 'youth', os.path.join( outdir, site, "ssaga" ) ]
        elif 'Parent_SAAGAv3' in path:
            command_array += [ path, 'parent', os.path.join( outdir, site, "ssaga" ) ]
        else:
            slog.info(subject_label, 'ERROR: could not determine whether the path contains Youth or Parent SSAGA',
                      path=str(path))
        run_converter(site, subject_label, command_array, verbose)

    elif verbose : 
        print "Warning: No conversion for file found!"

#
# Function: handle updated file by dispatching to the correct subhandler
#
def handle_file_update( path, verbose ):
    # First, let's get the site ID from the path
    match_site = re.search( 'ncanda/([A-Za-z]*)[^/]*/(.*)', path )

    if match_site:
        # Get the site ID
        site = match_site.group( 1 )
        # We do not accept data from the "admin" machines - testing only, not a collection site
        if site == 'admin':
            return

        filename = re.search( '(.*)/([^/].*)', path ).group( 2 );
        handle_file( path, site, filename, verbose)
                            
#
# Callback: catch files added and updated since last svn update
#
def notify( event_dict ):
    global updated_files
    if event_dict['kind'] == pysvn.node_kind.file:
        if event_dict['action'] == pysvn.wc_notify_action.update_add or event_dict['action'] == pysvn.wc_notify_action.update_update or event_dict['action'] == pysvn.wc_notify_action.restore:
            updated_files.append( event_dict['path'] )

#
# Main function: perform svn update and catch all resulting events
#
if args.verbose :
    print "Run svn update ..." 

dated_download = "" 
if args.date : 
    dated_download = args.date

if args.all : 
    date_tomorrow = datetime.date.today() + datetime.timedelta(days=1)
    date_beginning = datetime.date(2012,01,01)
    dated_download =  "{" + str(date_beginning) + "}:{" + str(date_tomorrow) + "}"

if args.last_week : 
    date_tomorrow = datetime.date.today() + datetime.timedelta(days=1)
    date_last_week = date_today - datetime.timedelta(weeks=1)
    dated_download =  "{" + str(date_last_week) + "}:{" + str(date_tomorrow) + "}"

if dated_download : 
    # Files that are added and deleted during the time period are now listed ! 
    cmd ='cd {}; svn diff --no-auth-cache --username {} --password \'{}\' --summarize -r {} | grep -v "D     "  | tr " " "," | cut -d, -f8 | grep "\." | tr "\n" ","'.format( svndir, session.api['svn_laptop']['user'], session.api['svn_laptop']['password'], dated_download )
    process = subprocess.Popen(cmd,shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE)
    out_std,out_err = process.communicate()
    if out_err :
        slog.info(hashlib.sha1('harvester' + out_err).hexdigest()[0:6],
                  "ERROR: Failed to update files in range " + args.date + ". Continued without them!",
                  script='harvester',
                  msg=out_err)
    else :
        if out_std :
            for fName in out_std[:-1].split(',') :
                updated_files.append(os.path.join(svndir,fName))               
        
elif args.file_to_upload:
    if not os.path.exists(args.file_to_upload):
        slog.info(hashlib.sha1('harvester'+args.file_to_upload).hexdigest()[0:6],
                  "ERROR: File does not exist!",
                  file=args.file_to_upload,
                  harvester_cmd=" ".join(sys.argv))
        sys.exit(1)

    updated_files.append(args.file_to_upload)

else : 
    if not session.run_svn('update', callbackNotifyFct = notify) : 
       sys.exit(1)

if args.verbose :
    print "... done " 
    print "Files to be uploaded :\n" + "\n".join(updated_files)
    print "Number of records uploading: ", len(updated_files)
 
# Append single file to upload

# Process all updated or added files
for file in updated_files:
    handle_file_update( file , args.verbose)

slog.takeTimer1("script_time","{'records': " + str(len(updated_files)) + "}")



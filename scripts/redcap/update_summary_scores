#!/usr/bin/env python

##
##  See COPYING file distributed along with the ncanda-data-integration package
##  for the copyright and license terms
##

import os
import re
import sys
import hashlib
import argparse

import pandas
import redcap
import sibispy
from sibispy import sibislogger as slog

import scoring

#
# Functions 
#

# Find all matching fields from a list of field names that match a list of given regular expression patterns
def get_matching_fields(field_list, pattern_list):
    matches = set()
    for field_pattern in pattern_list:
        pattern_matches = [field for field in field_list if re.match(field_pattern, field)]
        if len(pattern_matches) > 0:
            matches.update(pattern_matches)
        else:
            # If no matches, assume this is a "complete" field and simply add the pattern itself
            matches.update([field_pattern])
    return matches


def mark_missing(row, instrument):
    if row['%s_missing' % instrument] > 0:
        return 0
    else:
        return row['%s_complete' % instrument]


#
# Main 
#

# Setup command line parser
parser = argparse.ArgumentParser(description="Update longitudinal project forms"
                                             " from data imported from the data capture laptops",
                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument("-v", "--verbose",
                    help="Verbose operation",
                    action="store_true")
parser.add_argument("-i", "--instruments",
                    help="Select specific instruments to update. Separate multiple forms with commas.",
                    action="store", default=None)
parser.add_argument("-s", "--subject_id",
                    help="Only run for specific subject (multiple subject seperate with ',') .",
                    action="store", default=None)
parser.add_argument("-a", "--update-all",
                    help="Update all summary records, regardless of current completion status "
                         "(otherwise, only update records where incoming data completion status "
                         "exceeds existing summary data status)",
                    action="store_true")
parser.add_argument("-n", "--no-upload",
                    help="Do not upload any scores to REDCap server; instead write to CSV file with given path.",
                    action="store")
parser.add_argument("-p", "--post-to-github", help="Post all issues to GitHub instead of std out.", action="store_true")
parser.add_argument("-t","--time-log-dir",
                    help="If set then time logs are written to that directory",
                    action="store",
                    default=None)
args = parser.parse_args()

slog.init_log(args.verbose, args.post_to_github,'NCANDA REDCap', 'update_summary_scores', args.time_log_dir)
slog.startTimer1()

global count_uploaded
count_uploaded = 0

# First REDCap connection for the Summary project (this is where we put data)
session = sibispy.Session()
if not session.configure():
    if args.verbose:
        print "Error: session configure file was not found"

    sys.exit()


# If connection to redcap server fail, try multiple times
try:
    rc_summary =  session.connect_server('data_entry', True)
except Exception as e:
    slog.info(hashlib.sha1('update_summary_scores').hexdigest()[0:6],
    "ERROR: Could not connect to redcap!: {0}".format(e),
    script = 'update_summary_scores')
    sys.exit()

form_event_mapping = rc_summary.export_fem(format='df')

# Get record IDs and exclusions
demographics_fields = ['study_id', 'dob', 'sex']
demographics = rc_summary.export_records(fields=demographics_fields, event_name='unique', format='df').dropna()
demographics = pandas.concat([demographics.xs(event, level=1) for event in ['baseline_visit_arm_1', 'baseline_visit_arm_4']])

# If list of forms given, only update those
instrument_list = scoring.instrument_list
if args.instruments:
    instrument_list = []
    for inst in args.instruments.split(','):
        if inst in scoring.instrument_list:
            instrument_list.append(inst)
        else:
            print "WARNING: no instrument with name '%s' defined.\n" % inst

# Import scoring module - this has a list of all scoring instruments with input fields, scoring functions, etc.
for instrument in instrument_list:
    if args.verbose:
        print 'Scoring instrument', instrument

    # Get fields in the summary project for this instrument
    fields_list = ['%s_complete' % instrument]
    if args.subject_id:
        record_ids = rc_summary.export_records(fields=fields_list, records=[args.subject_id],event_name='unique', format='df')
    else : 
        record_ids = rc_summary.export_records(fields=fields_list,event_name='unique', format='df')

    # Get events for which this instrument is present, and drop all records from other events
    instrument_events_list = form_event_mapping[form_event_mapping['form_name'] == scoring.output_form[instrument]]['unique_event_name'].tolist()

    record_ids = record_ids[record_ids.index.map(lambda x: x[1] in instrument_events_list)]
    # Unless instructed otherwise, drop all records that already exist
    if not args.update_all:
        record_ids = record_ids[record_ids['%s_complete' % instrument].map(lambda x: True if str(x) == 'nan' else x < 1)]

    if len(record_ids):
        if args.verbose:
            print len(record_ids), 'records to score'

        # Now get the imported records referenced by each record in the summary table
        import_fields = []
        for import_instrument in scoring.fields_list[instrument].keys():
            import_fields += get_matching_fields(rc_summary.field_names, scoring.fields_list[instrument][import_instrument])

        # Retrieve data from record in chunks of 50 records
        # We cannot always get everything in one request (too large), but don't want each record by itself either, for speed.
        # Have to do this separately for each event, because of the way REDCap separates study ID and event name in the request
        imported = []
        for event_name in set(record_ids.index.map(lambda key: key[1]).tolist()):
            records_this_event = record_ids.xs( event_name, level=1).index.tolist()
            for idx in xrange(0, len(records_this_event), 50):
                imported.append(rc_summary.export_records(fields=import_fields,
                                                          records=records_this_event[idx:idx + 50],
                                                          events=[event_name], event_name='unique', format='df'))
       
        scored_records = scoring.compute_scores(instrument,pandas.concat(imported), demographics) 
        len_scored_records = len(scored_records)
    
        if not len_scored_records : 
            continue

        if args.verbose:
            print len_scored_records, 'scored records to upload'

        if args.no_upload:
            scored_records.to_csv(args.no_upload)
        else:
            uploaded = session.redcap_import_record(instrument, None, None, 'upload_instruments', scored_records)
            if not uploaded : 
                continue 

            if 'count' in uploaded.keys() and uploaded['count'] > 0:
                count_uploaded += uploaded['count']               
                if args.verbose:
                    print 'Updated', uploaded['count'], 'records of "%s"' % instrument
            elif args.verbose:
                print 'No updates for instrument "%s"' % instrument, uploaded
    else:
        if args.verbose:
            print 'No unscored records instrument "%s"' % instrument

slog.takeTimer1("script_time","{'records': " + str(len(instrument_list)) + ", 'uploads': " +  str(count_uploaded) + "}")

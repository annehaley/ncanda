#!/usr/bin/env python

##
##  See COPYING file distributed along with the ncanda-data-integration package
##  for the copyright and license terms
##

import os
import re
import sys
import argparse
import datetime
import hashlib

import yaml
import pandas
import redcap
import pyxnat
import sibis

import export_mr_sessions_pipeline as mrpipeline
import export_redcap_to_pipeline as rcpipeline
import redcap_form_locker as rclocker

import sibispy
from sibispy import sibislogger as slog

# Time format
date_format_ymd = '%Y-%m-%d'

# Setup command line parser
parser = argparse.ArgumentParser( description="For all subjects and visits in the REDCap database, export demographics, clinical measures, NP test scores, and other configured information into the pipeline directory tree.", formatter_class=argparse.ArgumentDefaultsHelpFormatter )
parser.add_argument( "-v", "--verbose", help="Verbose operation", action="store_true")
parser.add_argument( "--locked_form_report", help="Output a report indicating locked forms ", action="store_true")
parser.add_argument( "--site", help="Limit export by site (SRI, UCSD, OHSU, UPMC, DUKE). Multiple sites can be listed, separated by comma (',')", action="store", default=None )
parser.add_argument( "--events", help="Limit export by event labels (e.g., 'baseline_visit_arm_1'). Multiple events can be listed, separated by comma (',')", action="store", default=None )
parser.add_argument( "--subject", help="Limit export by subject site id (e.g., 'X-12345-X-9').", action="store", default=None )
parser.add_argument( "--export", help="Limit export by output file (e.g., 'cddr'; do not include '.txt' suffix). Multiple exports can be listed, separated by comma (',')", action="store", default=None )
parser.add_argument( "-e", "--exclude", help="Exports Meausres for excluded subjects", action="store_true" )
parser.add_argument( "--datadict-dir", help="Provides a directory in which the script creates data dictionaries for all supported export files.", action="store", default=None )
parser.add_argument( "pipelinedir", help="Root directory of the image analysis pipeline.", action="store")
parser.add_argument("-p", "--post-to-github", help="Post all issues to GitHub instead of std out.",action="store_true")
parser.add_argument("-t", "--time-log-dir",
                    help="If set then time logs are written to that directory (e.g. /fs/ncanda-share/ncanda-data-log/crond)",
                    action="store",
                    default=None)
args = parser.parse_args()

slog.init_log(args.verbose, args.post_to_github, 'NCANDA ', 'export_measures',
                args.time_log_dir)
slog.startTimer1()

global uploads
uploads = 0

# Open connection with REDCap server
session = sibispy.Session()
if not session.configure():
    if args.verbose:
        print "Error: session configure file was not found"

    sys.exit()

# Site changes are mapped here to the correct case identifier.
sibis_config = session.get_operations_dir()
special_cases_file = os.path.join(sibis_config, 'special_cases.yml')
if not os.path.isfile(special_cases_file) :
    slog.info("export_measures","Error: The following file does not exist :" + special_cases_file)
    sys.exit(1)

# Get a map of subjects that changed sited
# SB: Followed example in import_mr_sessions
with open(os.path.join(sibis_config, 'special_cases.yml')) as fi:
    site_change_map = yaml.load(fi).get('site_change')
    export_measures_map = site_change_map.get('export_measures')

try:
    redcap_project = session.connect_server('data_entry', True)
except redcap.RedcapError as e:
    error = "Error creating object redcap.Project()"
    slog.info(hashlib.sha1('export_measures').hexdigest()[0:6], error,
                  script='export_measures',
                  msg=str(e))
    sys.exit()



form_event_mapping = redcap_project.export_fem( format='df' )

# Organize REDCap metadata, e.g., filter all confidential fields out of export lists, and make code-to-label lookup dictionaries
rcpipeline.organize_metadata( redcap_project.metadata )

# If a directory for creating data dictionaries is given, make those first
if args.datadict_dir:
    rcpipeline.create_datadicts( args.datadict_dir )

# Open a connection with REDCap MySQL server.
if args.locked_form_report:
    cfg = os.path.join(os.path.expanduser("~"), '.server_config/redcap-mysql.cfg')
    engine = rclocker.create_connection(cfg)
    if args.verbose:
        print "Configured to output a report of locked forms..."
        print "Connected to REDCap MySQL: {0}".format(engine)

# Open connection with XNAT server
xnat = pyxnat.Interface( config = os.path.join( os.path.expanduser("~"), '.server_config/ncanda.cfg' ) )

#
# Get subject and project IDs
#
try:
    subject_project_list = xnat.select( 'xnat:subjectData', ['xnat:subjectData/SUBJECT_LABEL', 'xnat:subjectData/SUBJECT_ID','xnat:subjectData/PROJECT'] ).where( [ ('xnat:subjectData/SUBJECT_LABEL','LIKE', '%')] ).items()
except:
    sys.exit( "ERROR: retrieving subject list from XNAT failed." )

subject_project_dict = dict()
subject_label_to_sid_dict = dict()
for ( subject_label, subject_id, projects ) in subject_project_list:
    subject_project_dict[subject_label] = ( subject_id, projects )
    subject_label_to_sid_dict[subject_label] = subject_id

# Function to get a subject's next visit - this is so we can exclude MRI collected after the next visit date, but still within N days
def get_subject_next_visit_date( subject, after_visit_date ):
    subject_visit_dates = visit_log_redcap.xs( subject, level=0 )['visit_date'].dropna()
    subject_visit_dates = subject_visit_dates[ subject_visit_dates.index.map( lambda key: key != 'recovery_baseline_arm_2' ) ] # Exclude "Recovery" baseline from list - this will usually be the MR day of a normal visit
    later_visits_this_subject = sorted( [ date for date in subject_visit_dates.tolist() if date > after_visit_date ] )
    if len( later_visits_this_subject ) > 0:
        return later_visits_this_subject[0]
    else:
        return None

#
# Number of days between two dates (signed value is returned - negative if second date is before first)
#
def days_between_dates( date_from_str, date_to_str, date_format=date_format_ymd ):
    return (datetime.datetime.strptime( date_to_str, date_format_ymd ) - datetime.datetime.strptime( date_from_str, date_format_ymd ) ).days

#
# Main program loop
#
baseline_events = ['baseline_visit_arm_1','baseline_visit_arm_4']
subject_fields = ['study_id', 'dob',  'exclude', 'enroll_exception',
                  'siblings_enrolled', 'siblings_id1', 'hispanic', 'race',
                  'race_other_code']
subject_data = redcap_project.export_records(fields=subject_fields,
                                             events=baseline_events,
                                             event_name='unique',
                                             format='df')
subject_data = pandas.concat([subject_data.xs(event, level=1) for event in baseline_events])
subject_data['siblings_id1'] = subject_data['siblings_id1'].map( lambda x: subject_label_to_sid_dict[x] if x in subject_label_to_sid_dict.keys() else '' )

visit_log_fields = ['study_id', 'redcap_data_access_group', 'visit_date',
                    'mri_qa_completed', 'mri_t1_age', 'mri_dti_age',
                    'mri_rsfmri_age','mri_scanner', 'visit_ignore']
visit_log_redcap = redcap_project.export_records(fields=visit_log_fields,
                                                 event_name='unique',
                                                 export_data_access_groups=True,
                                                 format='df')

if args.site:
    sites = args.site.lower().split( ',' )
    visit_log_redcap = visit_log_redcap[ visit_log_redcap['redcap_data_access_group'].map( lambda dag: dag in sites ) ]

forms_by_event_dict = dict()
for event in redcap_project.events:
    event_id = event['unique_event_name']
    forms_by_event_dict[event_id] = set( form_event_mapping[form_event_mapping['unique_event_name'] == event_id ]['form_name'].tolist() )

# Filter out all excluded subjects
if args.exclude:
    visit_log_redcap = visit_log_redcap[ visit_log_redcap.index.map( lambda key: False if subject_data['exclude'][key[0]] != 1 else True ) ]
    visit_log_redcap = visit_log_redcap[~visit_log_redcap.visit_date.isnull()]
else:
    visit_log_redcap = visit_log_redcap[ visit_log_redcap.index.map( lambda key: False if subject_data['exclude'][key[0]] == 1 else True ) ]

# Limit to selected events
if args.events:
    events = args.events.lower().split( ',' )
    visit_log_redcap = visit_log_redcap[ visit_log_redcap.index.map( lambda key: key[1] in events ) ]

# Filter out events not yet supported by the pipeline exporter
visit_log_redcap = visit_log_redcap[ visit_log_redcap.index.map( lambda key: key[1] in mrpipeline.event_lookup.keys() ) ]

# Limit to selected export files
if args.export:
    select_exports = args.export.lower().split( ',' )
else:
    select_exports = None

if args.subject:
    visit_log_redcap = visit_log_redcap.loc[[args.subject]]

if args.verbose:
    print "Exporting %d REDCap records." % len( visit_log_redcap )

# Iterate over all remaining rows
for [key,row] in visit_log_redcap.iterrows():
    if key[0] not in subject_label_to_sid_dict.keys():
        if args.verbose:
            print "Missing XNAT ID for subject",key[0]
        continue 
    # only do sessions that are not ignored 
    if row['visit_ignore___yes'] :
        continue 

    subject_xnat_id = subject_label_to_sid_dict[key[0]]
    visit_date = str(row['visit_date'])
    if visit_date == 'nan':
        if args.verbose:
            print "Missing '%s' visit date for subject %s" % ( key[1], key[0] )
    else:
            this_subject_data = subject_data.ix[key[0]]
            subject_dob_str = str( this_subject_data['dob'] )
            if not re.match( '[0-9]{4}-[0-9]{2}-[0-9]{2}', subject_dob_str ):
                error = "Missing or invalid birthdate '%s' for subject %s" % ( subject_dob_str, key[0] )
                slog.info(str(key[0]), error)
            else:
                visit_age = days_between_dates( subject_dob_str, visit_date ) / 365.242

                # If exclude flag is set, this will override the excludes value and export excluded subjects
                if args.exclude:
                    excludes = 0.0
                else:
                    excludes = 1

                # Check if pipeline directory given and export imaging series there
                if (this_subject_data['exclude'] != excludes):
                    (arm_code,visit_code,subject_datadir_rel) = (None, None, None)
                    try:
                        (arm_code,visit_code,subject_datadir_rel) = mrpipeline.translate_subject_and_event( subject_xnat_id, key[1] )
                    except:
                        slog.info(hashlib.sha1('export_measures').hexdigest()[0:6], "Event",key[1],"is not supported yet.",
                                      info = "Add event to ncanda_operations/setup.yml")
                        continue
                        
                    if arm_code:
                        subject_datadir = os.path.join(args.pipelinedir,
                                                       subject_datadir_rel)

                        # map the redcap subject key to readable vars
                        redcap_subject, redcap_event = key
                        site = redcap_subject[0]

                        # Handle mapping demographic info for subjects that
                        # changed site
                        if redcap_subject in export_measures_map.iterkeys():
                            # Only set for visits in the past
                            visit_case_id_map = export_measures_map.get(
                                redcap_subject)
                            if visit_code in visit_case_id_map.iterkeys():
                                case_id_map = visit_case_id_map.get(visit_code)
                                # Use correct case id (NCANDA_S00001)
                                subject_code = case_id_map.get('subject')
                            else:
                                # update with default info for future events
                                case_id_map = visit_case_id_map.get('default')
                                subject_code = case_id_map.get('subject')
                                site = case_id_map.get('site')
                            # Make sure results goto the correct directory
                            subject_datadir = subject_datadir.replace(
                                subject_xnat_id, subject_code)
                            subject_xnat_id = subject_code

                        uploads += 1

                        # Export measures from RECap into the pipeline.
                        rcpipeline.export(redcap_project,
                                          site,
                                          redcap_subject,
                                          redcap_event,
                                          this_subject_data,
                                          visit_age,
                                          row,
                                          arm_code,
                                          visit_code,
                                          subject_xnat_id,
                                          subject_datadir,
                                          forms_by_event_dict[key[1]],
                                          select_exports=select_exports,
                                          verbose=args.verbose)

                        # Write report of forms locked for this subject, arm,
                        # visit
                        if args.locked_form_report and arm_code == 'standard':
                            if args.verbose:
                                print "Creating a report of locked forms for: " \
                                      "{0}, {1}, {2}".format(subject_xnat_id, arm_code, visit_code)
                            # Link directory names with names in REDCap mysql
                            if visit_code == 'baseline' : 
                                visit = "Baseline visit"
                            else :
                                visit = visit_code.split('_',1)[1] + " visit"

                            # Get a dataframe of the locked forms
                            arm = "{0} Protocol".format(arm_code.capitalize())
                            
                            locked_forms = rclocker.report_locked_forms(key[0], subject_xnat_id,
                                                                        forms_by_event_dict[key[1]], 'ncanda_subject_visit_log',
                                                                        arm, visit, engine)
                            filename = os.path.join(os.path.abspath(subject_datadir), 'measures', 'locked_forms.csv')
                            rcpipeline.safe_csv_export(locked_forms, filename, verbose=args.verbose)
                            if args.verbose:
                                print "If locked form report changed from existing version than updated: {0}".format(filename)

slog.takeTimer1("script_time","{'Records': " + str(len(visit_log_redcap)) + ", 'Uploaded': " +  str(uploads) + "}")

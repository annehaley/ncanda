#!/usr/bin/env python

##
##  See COPYING file distributed along with the ncanda-data-integration package
##  for the copyright and license terms
##

import os
import re
import sys
import time
import datetime
import argparse
import hashlib
import ast

import yaml
import sibispy
from sibispy import sibislogger as slog
import pyxnat
import redcap
import pandas as pd

import import_mr_sessions_stroop as stroop
import export_mr_sessions_pipeline as mrpipeline
import export_redcap_to_pipeline as rcpipeline

#
# GLOBAL VARIABLES 
# 
date_format_ymd = '%Y-%m-%d'
today = time.strftime(date_format_ymd)

ncanda_scan_types = [ 't1spgr', 'mprage', 't2fse', 'dti6b500pepolar', 'dti60b1000', 'rsfmri' ]

#
# Functions 
# 
def get_sessions_in_range(redcap_visit_id,xnat, subject_label, project_id, subject_id, date_range_from, date_range_to, verbose):
    #print "get_sessions_in_range", redcap_visit_id,xnat, subject_label, project_id, subject_id, date_range_from, date_range_to
    sessions_in_range = []
    for session_id, session_subject_id, projects, date, scanner in xnat_sessions_list:
        if (subject_id == session_subject_id) and (date >= date_range_from) and (date <= date_range_to):
            sessions_in_range.append((session_id, projects, date))

    if not sessions_in_range:
        # handling special cases 
        if verbose: 
            print "  No session in range for ", subject_id, date_range_from, date_range_to

        # Outside visit window
        exception_list = exceptions_window.get(subject_id)
        if exception_list:
            for except_entry in exception_list.split(';'):
                [e_eid,e_visit] = except_entry.split(',')
                if (e_visit >= date_range_from) and (e_visit <= date_range_to):
                    for eid, session_subject_id, projects, date, scanner in xnat_sessions_list:
                        if eid == e_eid : 
                            if subject_id != session_subject_id:
                                error='The eid defined in outside_visit_window of special_cases.yml is not correct as subject_id associated with eid in xnat does not match the subject id associated with the eid in special_cases.yml'
                                slog.info(redcap_visit_id,error,
                                          expected_subject_id=subject_id,
                                          xnat_subject_id=session_subject_id,
                                          xnat_visit_id = e_eid
                                )
                                return sessions_in_range

                            else:
                                if verbose: 
                                    print "  Exception: Adding session", e_eid, projects, date 

                                sessions_in_range.append((e_eid, projects, date))

        # Session might have changed site - this is not necessary - bug in code 
        # if not sessions_in_range and subject_label in export_measures_map.iterkeys():
        #     orig_subject_id = export_measures_map.get(subject_label).get('default').get('xnat_subject_id')
        #     for session_id, session_subject_id, projects, date, scanner in xnat_sessions_list:
        #         if (orig_subject_id == session_subject_id) and (date >= date_range_from) and (date <= date_range_to):
        #             if verbose: 
        #                 print "  Exception: Session (", session_id, projects, date , ") changed site - used ", orig_subject_id ," to find session!"  
        #             sessions_in_range.append((session_id, projects, date))            

    return sessions_in_range


# Get URIs for spiral data (Stroop and resting state, where they exist)
def get_spiral_uris( xnat_eid_list ):
    spiral_uri = ''
    spiralrest_uri = ''
    for xnat_eid in xnat_eid_list:
        resource_dict_list = xnat._get_json( '/data/experiments/%s/resources/?format=json' %xnat_eid )
        for res in resource_dict_list:
            if 'spiral' in res['label'].lower():
                resource_id = res['xnat_abstractresource_id']
                eid = res['cat_id']
                obj = xnat._get_json('/data/experiments/%s/resources/%s/files?format=json' %(eid, resource_id))
                if len( obj ) > 0:
                    file_path = obj[0]['Name']
                    if 'rest' in res['label'].lower():
                        spiralrest_uri = "/".join([eid, resource_id, file_path])
                    else:
                        spiral_uri = "/".join([eid, resource_id, file_path])

    return (spiral_uri, spiralrest_uri)

def get_phantom_scans_for_date( date, scanner ):
    return [ session for (session,sscanner,sdate) in xnat_phantom_sessions_list if (sdate == date) and (sscanner == scanner) ]

def get_phantom_scans_for_date_24h( yesterday, tomorrow, scanner ):
    return [ session for (session,sscanner,sdate) in xnat_phantom_sessions_list if ((sdate == yesterday) or (sdate == tomorrow)) and (sscanner == scanner) ]

# Get one or more custom variables from XML representation of experiment
def get_custom_variables( experiment, field_names, default_value=None ):
    xml = experiment.get()
    values = []
    for field_name in field_names:
        field_regex = '.*<xnat:field name="%s">(.*?)</xnat:field>' % field_name.lower()
        match = re.match( field_regex, xml, flags=re.DOTALL )
        if match:
            values.append( re.sub( '\s*<!--.*?-->\s*', '', match.group( 1 ), flags=re.DOTALL ) )
        else:
            values.append( default_value )
    return values

#
# Get list of all usable scans in the experiments listed
#
def get_usable_scans_list( xnat, xnat_eid_list ):
    result = []

    for xnat_eid in xnat_eid_list:
        for scan in xnat.select.experiment( xnat_eid ).scans('*').get():
            ( type, quality ) = xnat.select.experiment( xnat_eid ).scan( scan ).attrs.mget(['type','quality'])
            if quality == 'usable':
                result.append( ( type, xnat_eid, scan ) )

    return result

#
# Number of days between two dates (signed value is returned - negative if second date is before first)
#
def days_between_dates( date_from_str, date_to_str, date_format=date_format_ymd ):
    return (datetime.datetime.strptime( date_to_str, date_format_ymd ) - datetime.datetime.strptime( date_from_str, date_format_ymd ) ).days


#
# Get scans from a list, selected by type
#

def get_scans_by_type( usable_scans_list, visit_date, redcap_visit_id):
    result = dict()
    for type in ncanda_scan_types:
          # Get list of scans for this type; as we go, compute and sort by number of days between visit date and scan date
        scans = sorted( [ (eid, scan, days_between_dates(visit_date,xnat_sessions_dict[eid][0])) for (scan_type,eid,scan) in usable_scans_list if re.match( '^ncanda-%s-v1$' % type, scan_type ) ], key=lambda x: abs(x[2]) )

        # Changed logic here should generally just be one scan across all sessions
        # if (len( scans ) == 1) or (len( scans ) > 1 and scans[0][2] < scans[1][2]): 
        if (len( scans ) == 1) :
            # either only one usable scan, or unique usable scan with lowest number of days after "visit_date"
            result[type] = scans[0]
        elif len(scans) > 1:
            slog.info(redcap_visit_id, 
                      "ERROR: more than one scan of type {} on the same visit ".format(type),
                      eid_and_scan_number=str(scans),
                      type = str(type))
    return result

#
# Get fieldmap series for a given scan series (either DTI or rs-fMRI) - this can be more than one, because Siemens writes two under the same type
#
def get_fieldmap_scans( usable_scans_list, for_scan, scannerMfg,redcap_visit_id, exceptions_num_type):
    matched_fieldmaps = [ (eid,scan) for (type,eid,scan) in usable_scans_list if 'grefieldmap' in type and for_scan[0]==eid ]

    # only point out if we have more usable scans than needed so it is correctly labelled in xnat
    # if they are not enough scans it should have been cought before otherwise need to add another round of exception /get error message twice
    # as it is originally checked in check_new_sessions
    exception_list = exceptions_num_type.get(redcap_visit_id) 
    if exception_list and 'grefieldmap' in exception_list:
        return matched_fieldmaps
        
    if scannerMfg == "SIEMENS":
        if len(matched_fieldmaps) > 2 : 
            slog.info(redcap_visit_id,"Error: More than two usable grefieldmaps!",
                      number_of_scans = str(len(matched_fieldmaps)),
                      xnat_visit_id = for_scan[0], 
                      scanner_mfg = scannerMfg)

    elif scannerMfg == "GE":
        if len(matched_fieldmaps) > 1 : 
            slog.info(redcap_visit_id,"Error: More than one usable grefieldmaps!",
                      number_of_scans = str(len(matched_fieldmaps)),
                      xnat_visit_id = for_scan[0], 
                      scanner_mfg = scannerMfg)

    elif len(matched_fieldmaps) > 0 : 
            slog.info(redcap_visit_id,"Error: Scanner manufacturer not known even though usable grefieldmap exists!",
                      number_of_scans = str(len(matched_fieldmaps)),
                      xnat_visit_id = for_scan[0], 
                      scanner_mfg = scannerMfg)
            return []
        
    return matched_fieldmaps

#
# Encode session and scan in a single string
#
def encode_session_and_scan( session_and_scan, type_list ):
    for type in type_list:
        if type in session_and_scan.keys():
            return "%s/%s" % (session_and_scan[type][0],session_and_scan[type][1])
    return ""

#
# Get data from XNAT for one subject/visit date
#
#  Returned result is a dictionary of keys and values to enter into the REDCap record for this subject/visit
#

def get_xnat_data(redcap_key, xnat, xnat_sid, xnat_pid, pipe_id, visit_date, date_of_birth,
                  next_visit_date, mri_inspection_completed, verbose=None):
    subject = redcap_key[0]
    event = redcap_key[1]

    result = dict()

    # Initialize result as "nothing there"
    result['mr_session_report_complete'] = ''
    result['mri_xnat_sid'] = ''
    result['mri_xnat_eids'] = ''
    result['mri_qa_completed'] = ''
    result['mri_t1_date'] = ''
    result['mri_t1_age'] = ''
    result['mri_dti_date'] = ''
    result['mri_dti_age'] = ''
    result['mri_rsfmri_date'] = ''
    result['mri_rsfmri_age'] = ''
    result['mri_notes'] = ''
    result['mri_adni_phantom'] = ''
    result['mri_adni_phantom_eid'] = ''

    result['mri_series_t1'] = ''
    result['mri_series_t2'] = ''
    result['mri_series_dti6b500pepolar'] = ''
    result['mri_series_dti60b1000'] = ''
    result['mri_series_dti_fieldmap'] = ''
    result['mri_series_rsfmri'] = ''
    result['mri_series_rsfmri_fieldmap'] = ''
    result['mri_scanner'] = ''

    # First, see if this subject is in the XNAT database and has a scan session
    # for the given date or later
    visit_date_plusNd = (datetime.datetime.strptime( visit_date, date_format_ymd) + datetime.timedelta(args.max_days_after_visit)).strftime(date_format_ymd)
    # If we have a date for the next visit, and it's earlier than +N days (could
    # be for "Recovery" final visit), then use that date instead
    if next_visit_date and next_visit_date < visit_date_plusNd:
        visit_date_plusNd = next_visit_date
        if args.verbose:
            print "Due to next visit date being within visit date range, the visit date range is shortened to (", visit_date , ", " , visit_date_plusNd +")."  

    if args.verbose:
        print("Checking {} for {} with visit date {} to {}".format(subject,
                                                                   event,
                                                                   visit_date,
                                                                   visit_date_plusNd))

    # Get project and subject IDs for this subject by name
    if subject not in subject_project_dict.keys():
        if args.verbose:
            print("Subject {} cannot be found in XNAT.".format(subject))
        return result

    redcap_visit_id=subject+"-"+visit_date

    # Get all experiments for this subject within the given range of the visit
    # date
    xnat_session_data = get_sessions_in_range(redcap_visit_id,xnat,
                                              subject,
                                              xnat_pid,
                                              xnat_sid,
                                              visit_date,
                                              visit_date_plusNd,
                                              args.verbose)


    if verbose:
        print "XNAT Session Data:", xnat_session_data

    if not xnat_session_data:
        if today > visit_date_plusNd:
            
            error='Missing MRI for Subject with visit date.'
            slog.info(redcap_visit_id,error,
                          project_id=xnat_pid,
                          visit_id=event,
                          visit_date=visit_date)
        return result


    # Get the experiment IDs for all sessions in the given range
    xnat_eid_list = [session for (session, project, date) in xnat_session_data]
    if not len(xnat_eid_list):
        return result

    result['mri_xnat_sid'] = pipe_id

    # Add New Variables 
    # this failed a lot 
    # scanner_model_str = 'xnat:mrSessionData/scanner/model'
    # scanner_model = xnat.select.experiment(xnat_eid).attrs.get(scanner_model_str)
    # scanner_mfc_str = 'xnat:mrSessionData/scanner/manufacturer'
    # scanner_mfc = xnat.select.experiment(xnat_eid).attrs.get(scanner_mfc_str)
    # Does not work if the scanner label is actually not defined  
    # field_regex = '.*<xnat:scanner manufacturer="(.*?)" model="(.*?)">(.*?)</xnat:scanner>'
    first_eid = xnat_eid_list[0]
    field_regex = '.*<xnat:scanner manufacturer="(.*?)" model="(.*?)"'
    match = re.match(field_regex, xnat.select.experiment(first_eid).get(), flags=re.DOTALL)
    scannerMfc = "" 
    if match:
        scannerMfc = match.group(1).upper().split(" ",1)[0]
        result['mri_scanner'] = match.group(1) + " " + match.group(2) + " " + str(xnat.select.experiment(first_eid).attrs.get('scanner'))



    # For all experiments we have found, get list of all "usable" scans
    usable_scans_list = get_usable_scans_list(xnat, xnat_eid_list)
    # Now each of the standard NCANDA scans from the usable list - select
    # closes to "visit_date" if there is more than one for a given type
    scans_by_type = get_scans_by_type(usable_scans_list, visit_date, redcap_visit_id)

    result['mri_series_t1'] = encode_session_and_scan(scans_by_type, ['t1spgr', 'mprage'])
    result['mri_series_t2'] = encode_session_and_scan(scans_by_type, ['t2fse'])
    result['mri_series_dti6b500pepolar'] = encode_session_and_scan(scans_by_type, ['dti6b500pepolar'])

    # Get fieldmap series for the DTI and the rs-fMRI scans
    if 'dti60b1000' in scans_by_type.keys():
        result['mri_series_dti60b1000'] = encode_session_and_scan( scans_by_type, ['dti60b1000'])
        # only choose gradient map that was acquired in the same session as dti scan
        fieldmaps_dti60 = get_fieldmap_scans(usable_scans_list, scans_by_type['dti60b1000'],scannerMfc,redcap_visit_id, exceptions_num_type)
        result['mri_series_dti_fieldmap'] = ' '.join(['%s/%s' % (eid,scan) for (eid, scan) in fieldmaps_dti60])

    if 'rsfmri' in scans_by_type.keys():
        result['mri_series_rsfmri'] = encode_session_and_scan( scans_by_type, ['rsfmri'])
        # only choose gradient map that was acquired in the same session as fmri scan
        fieldmaps_rsfmri = get_fieldmap_scans( usable_scans_list, scans_by_type['rsfmri'],scannerMfc,redcap_visit_id, exceptions_num_type)
        result['mri_series_rsfmri_fieldmap'] = ' '.join(['%s/%s' % (eid,scan) for (eid, scan) in fieldmaps_rsfmri])

    for xnat_eid in xnat_eid_list:
        result['mri_xnat_eids'] += xnat_eid + ' '
        note = xnat.select.experiment(xnat_eid).attrs.get('note')
        experiment_note = note.strip()
        if len(experiment_note) > 0:
            result['mri_notes'] += '[%s] %s ' % (xnat_eid, re.sub('&quot;', '"', experiment_note))

    # Take off extra spaces
    result['mri_xnat_eids'] = result['mri_xnat_eids'].strip()
    result['mri_notes'] = result['mri_notes'].strip()

    # Get date and age for DTI scan (use DTI60 because DTI6 may be bad or missing, and fieldmap used instead)
    if 'dti60b1000' in scans_by_type.keys():
        result['mri_dti_date'] = xnat_sessions_dict[scans_by_type['dti60b1000'][0]][0]
        result['mri_dti_age'] = str(days_between_dates( date_of_birth, result['mri_dti_date']) / 365.242)

    # Get date and age for resting-state fMRI scan
    if 'rsfmri' in scans_by_type.keys():
        result['mri_rsfmri_date'] = xnat_sessions_dict[scans_by_type['rsfmri'][0]][0]
        result['mri_rsfmri_age'] = str(days_between_dates(date_of_birth, result['mri_rsfmri_date']) / 365.242)

    # Get the uri's for the spiral task and spiral resting state
    (result['mri_eid_spiral_stroop'], result['mri_eid_spiral_rest']) = get_spiral_uris(xnat_eid_list)

    # Get the T1w data (MPRAGE or SPGR) and its associated ADNI phantom scan
    if 't1spgr' in scans_by_type.keys():
        t1w_experiment_data = xnat_sessions_dict[scans_by_type['t1spgr'][0]]
    elif 'mprage' in scans_by_type.keys():
        t1w_experiment_data = xnat_sessions_dict[scans_by_type['mprage'][0]]
    else:
        t1w_experiment_data = None

    if t1w_experiment_data:
        # Compute age at T1w scan
        result['mri_t1_date'] = t1w_experiment_data[0]
        result['mri_t1_age'] = str(days_between_dates( date_of_birth, t1w_experiment_data[0]) / 365.242)

        # Find ADNI phantom scan in appropriate date range (either same day or within 24h)
        # Found same-day phantom scan?
        phantom_scans = get_phantom_scans_for_date(t1w_experiment_data[0], t1w_experiment_data[1])
        if len(phantom_scans):
            result['mri_adni_phantom'] = '1' # Found same day
            result['mri_adni_phantom_eid'] = phantom_scans[0]
        else:
            # No - look one day before and after
            this_date = datetime.datetime.strptime( t1w_experiment_data[0], date_format_ymd)
            tomorrow = (this_date + datetime.timedelta(1)).strftime(date_format_ymd)
            yesterday = (this_date - datetime.timedelta(1)).strftime(date_format_ymd)

            # Search for all sessions on previous day after given time, or next day before given time (we already know there isn't a scan on the same date)
            phantom_scans = get_phantom_scans_for_date_24h(yesterday, tomorrow, t1w_experiment_data[1])

            # Found it now or not?
            if len(phantom_scans):
                result['mri_adni_phantom'] = '2'  # Found 24h
                result['mri_adni_phantom_eid'] = phantom_scans[0]
            else:
                result['mri_adni_phantom'] = '3'  # Missing

    # Get custom variables of the "Reading" group
    t1w_experiment = xnat.select.project( xnat_pid ).subject( xnat_sid ).experiment( re.sub('/.*', '', result['mri_series_t1']))
    [ result['mri_datetodvd'], result['mri_findingsdate'], result['mri_findings'],
      result['mri_excludefromanalysis'], result['mri_referredtopi']] = get_custom_variables( t1w_experiment, [ 'DateToDVD', 'FindingsDate', 'Findings', 'ExcludeFromAnalysis', 'ReferredToPI'], '')

    # Check if images have received reading and have either been labeled 'normal' or marked as checked
    if (result['mri_findings'].lower() == 'normal') or mri_inspection_completed:
        result['mri_qa_completed'] = '1'
    else:
        result['mri_qa_completed'] = '0'

    # Set completion status of REDCap form based on whether QA is done (all sessions/scans looked at and no showstoppers in clinical reading)
    all_sessions_and_scans_used = [ sscn for sscn in [ result['mri_series_t1'], result['mri_series_t2'], result['mri_series_dti6b500pepolar'], result['mri_series_dti60b1000'],
                                                       result['mri_series_dti_fieldmap'], result['mri_series_rsfmri'], result['mri_series_rsfmri_fieldmap'] ] if sscn != '' ]
    if len( all_sessions_and_scans_used ) > 0:
        mandatory_fields = [ result[f] for f in ['mri_adni_phantom', 'mri_adni_phantom_eid', 'mri_series_t1', 'mri_series_t2', 'mri_series_dti6b500pepolar', 'mri_series_dti60b1000',
                                                 'mri_series_dti_fieldmap', 'mri_series_rsfmri', 'mri_series_rsfmri_fieldmap'] ]
        if (result['mri_qa_completed'] == '1') and not ('' in mandatory_fields):
            result['mr_session_report_complete'] = '2'
        else:
            result['mr_session_report_complete'] = '1'
    else:
        result['mr_session_report_complete'] = '0'

    # Return the result
    return result


# Function to get a subject's next visit - this is so we can exclude MRI
# collected after the next visit date, but still within N days
def get_subject_next_visit_date(subject, after_visit_date):
    subject_visit_dates = visit_log_redcap.xs( subject, level=0 )['visit_date'].dropna()
    subject_visit_dates = subject_visit_dates[ subject_visit_dates.index.map( lambda key: key != 'recovery_baseline_arm_2' ) ] # Exclude "Recovery" baseline from list - this will usually be the MR day of a normal visit
    later_visits_this_subject = sorted( [ date for date in subject_visit_dates.tolist() if date > after_visit_date ] )
    if len( later_visits_this_subject ) > 0:
        return later_visits_this_subject[0]
    else:
        return None


def get_sibling_id1(subject_label):
    if subject_label in subject_label_to_sid_dict.keys():
        result = subject_label_to_sid_dict[subject_label]
    else:
        result = ''
    return result


# ----------------------------------------
# MAIN 
# ----------------------------------------

# Setup command line parser
parser = argparse.ArgumentParser(description="For all subjects and visits in the REDCap database, find MR sessions in XNAT",
                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter )
parser.add_argument("-v", "--verbose",
                    help="Verbose operation",
                    action="store_true")
parser.add_argument("--max-days-after-visit",
                    help="Maximum number of days the scan session can be after the entered 'visit date' in REDCap to be assigned to a given event.",
                    action="store",
                    default=120,
                    type=int)
parser.add_argument("--site",
                    help="Limit processing by site (SRI, UCSD, OHSU, UPMC, DUKE). Multiple sites can be listed, separated by comma (',')",
                    action="store",
                    default=None )
parser.add_argument("--study-id",
                    help="Limit processing to subject id (e.g., A-00000-F-1)",
                    action="store",
                    default=None)
parser.add_argument("--pipeline-root-dir",
                    help="Root directory of the image analysis pipeline. Newly detected imaging series will be converted to NIFTI files and put here for processing. ",
                    action="store")
parser.add_argument("--run-pipeline-script",
                    help="Run image processing pipeline if new files were exported.",
                    action="store")
parser.add_argument("-f", "--force-update",
                    help="Check all records in REDCap even if marked as complete -does not recreate image data if it already exists and did not change",
                    action="store_true")
parser.add_argument("--missing-only",
                     help="Only consider missing records in REDCap, rather than unverified ones also.",
                    action="store_true")
parser.add_argument("--force-update-stroop",
                    help="Update all Stroop records in REDCap, overwriting existing data",
                     action="store_true")
parser.add_argument("--no-stroop",
                    help="Do not check for, or upload, MRI Stroop results (ePrime files in XNAT)",
                    action="store_true")
parser.add_argument("-n", "--no-upload",
                    help="Only check correspondences; do not upload results to REDCap",
                    action="store_true")
parser.add_argument("-p", "--post-to-github", help="Post all issues to GitHub instead of std out.", action="store_true")

parser.add_argument("-t","--time-log-dir",
                    help="If set then time logs are written to that directory (e.g. /fs/ncanda-share/ncanda-data-log/crond)",
                    action="store",
                    default=None)

args = parser.parse_args()

if args.verbose:
    print args

slog.init_log(args.verbose, args.post_to_github,'NCANDA Pipeline Feeder Messages', 'import_mr_sessions', args.time_log_dir)
slog.startTimer1()

session = sibispy.Session() 
if not session.configure() :
    if args.verbose:
        print "Error: session configure file was not found"
 
    sys.exit()

redcap_project = session.connect_server('data_entry', True) 
if not redcap_project : 
    if args.verbose:
        print "Error: Could not connect to Redcap" 

    sys.exit()

# Open connection with XNAT server
xnat = session.connect_server('xnat',True) 
if not xnat : 
    if args.verbose:
        print "Error: Could not connect to XNAT" 

    sys.exit()

# yml = os.path.join(os.path.expanduser("~"), '.server_config/config.yml')
# with open(yml, 'r') as fi:
#    sibis_config = yaml.load(fi).get('operations')
sibis_config = session.get_operations_dir()
special_cases_file = os.path.join(sibis_config, 'special_cases.yml')
if not os.path.isfile(special_cases_file) : 
    slog.info("import_mr_sesions","Error: The following file does not exist :" + special_cases_file)
    sys.exit(1)
              
# Get a list of cases outside the visit window that should be included.
with open(os.path.join(sibis_config, 'special_cases.yml')) as fi:
    special_cases = yaml.load(fi)
    export_measures_map = special_cases.get('site_change').get('export_measures')
    exceptions_window = special_cases.get('outside_visit_window')
    exceptions_num_type = special_cases.get('more_of_type')

form_event_mapping = redcap_project.export_fem( format='df' )

# Organize REDCap metadata, e.g., filter all confidential fields out of export lists, and make code-to-label lookup dictionaries
rcpipeline.organize_metadata( redcap_project.metadata )

#
# Get subject and project IDs
#
subject_project_list = session.xnat_export_general( 'xnat:subjectData', ['xnat:subjectData/SUBJECT_LABEL', 'xnat:subjectData/SUBJECT_ID','xnat:subjectData/PROJECT'], [ ('xnat:subjectData/SUBJECT_LABEL','LIKE', '%')],"subject_list")
# do not perform if not subject_project_list : as this is also true for [] !
if subject_project_list == None: 
    if args.verbose:
        print "ERROR: retrieving subject list from XNAT failed."
    sys.exit(1)

subject_project_dict = dict()
subject_label_to_sid_dict = dict()
for subject_label, subject_id, projects in subject_project_list:
    subject_project_dict[subject_label] = (subject_id, projects)
    subject_label_to_sid_dict[subject_label] = subject_id

#
# Get all scan sessions in permissible date range for a given subject and visit
#
xnat_sessions_fields = ['xnat:mrSessionData/SESSION_ID','xnat:mrSessionData/SUBJECT_ID','xnat:mrSessionData/PROJECTS','xnat:mrSessionData/DATE','xnat:mrSessionData/SCANNER']
xnat_sessions_list = session.xnat_export_general( 'xnat:mrSessionData', xnat_sessions_fields, [ ('xnat:mrSessionData/SESSION_ID','LIKE', '%') ],"subject_session_data")
if xnat_sessions_list == None :
    if args.verbose : 
        print "ERROR: retrieving session list from XNAT failed."

    sys.exit(1) 

xnat_sessions_dict = dict()
for ( session_id, session_subject_id, projects, date, scanner ) in xnat_sessions_list:
    xnat_sessions_dict[session_id] = ( date, scanner, projects )


#
# Get ADNI phantom scans from XNAT
#
# try:
#     xnat_phantom_sessions_list = xnat.select( 'xnat:mrSessionData', ['xnat:mrSessionData/SESSION_ID','xnat:mrSessionData/SCANNER','xnat:mrSessionData/DATE'] ).where( [ ('xnat:mrSessionData/LABEL','LIKE', '%-99999-P-9-%') ] ).items()
# except:
#     sys.exit( "ERROR: retrieving phantom session list from XNAT failed." )
xnat_phantom_sessions_list = session.xnat_export_general('xnat:mrSessionData', ['xnat:mrSessionData/SESSION_ID','xnat:mrSessionData/SCANNER','xnat:mrSessionData/DATE'], [ ('xnat:mrSessionData/LABEL','LIKE', '%-99999-P-9-%') ],'phantom_session_data')
if xnat_phantom_sessions_list  == None :
    if args.verbose:
       print "ERROR: retrieving subject list from XNAT failed." 

    sys.exit(1)


#
# Main program loop
#
# these are the "baseline" events of the study arms that have them
baseline_events = ['baseline_visit_arm_1', 'baseline_visit_arm_4']

subject_data = session.redcap_export_records("subject_data",fields=['study_id','dob','exclude','enroll_exception','siblings_enrolled','siblings_id1'], events = baseline_events, event_name='unique', format='df')

if type(subject_data) == type(None) or subject_data.empty:
    if verbose: 
        print "No subjects selected" 
    sys.exit(1)

# Gather all the subject data.
subject_data = pd.concat([subject_data.xs(event, level=1)
                          for event in baseline_events])

# Get the sibling id
subject_data['siblings_id1'] = subject_data['siblings_id1'].map(get_sibling_id1)

visit_log_redcap = session.redcap_export_records("visit_log", 
                                                 fields=['study_id',
                                                         'redcap_data_access_group',
                                                         'visit_date', 'visit_ignore',
                                                         'mr_session_report_complete',
                                                         'mri_xnat_sid',
                                                         'mri_stroop_complete',
                                                         'mri_inspection',
                                                         'mri_missing'],
                                                 event_name='unique',
                                                 export_data_access_groups=True,
                                                 format='df')

if type(visit_log_redcap) == type(None) or visit_log_redcap.empty:
    if verbose: 
        print "No subjects selected" 
    sys.exit(1)

# Limit export by site.
if args.site:
    sites = args.site.lower().split( ',' )
    visit_log_redcap = visit_log_redcap[ visit_log_redcap['redcap_data_access_group'].map( lambda dag: dag in sites ) ]

# Limit export by site id (e.g., A-00000-F-1)
if args.study_id:
    visit_log_redcap = visit_log_redcap.xs(args.study_id)
    visit_log_redcap.reset_index(inplace=True)
    visit_log_redcap['study_id'] = args.study_id
    visit_log_redcap.set_index(['study_id', 'redcap_event_name'],
                               inplace=True)

# Select only events that have the "MRI Session Report" form
mri_events_list = form_event_mapping[form_event_mapping['form_name'] == 'mr_session_report' ]['unique_event_name'].tolist()
get_events = lambda x: x[1] in mri_events_list
events_filter = visit_log_redcap.index.map(get_events)
columns = ['visit_date', 'visit_ignore___yes', 'mr_session_report_complete',
           'mri_xnat_sid', 'mri_inspection___completed', 'mri_missing']
mr_sessions_redcap = visit_log_redcap[events_filter][columns]

forms_by_event_dict = dict()
for event in redcap_project.events:
    event_id = event['unique_event_name']
    forms_by_event_dict[event_id] = set( form_event_mapping[form_event_mapping['unique_event_name'] == event_id ]['form_name'].tolist() )

# Have pipeline feeder check for "excluded" subjects that might have previously entered the pipeline
if args.pipeline_root_dir:
    excluded_subjects = mr_sessions_redcap[ mr_sessions_redcap.index.map( lambda key: subject_data['exclude'][key[0]] == 1 ) ]['mri_xnat_sid'].dropna().tolist()
    mrpipeline.check_excluded_subjects( excluded_subjects, args.pipeline_root_dir )

# Filter out all records marked as "Complete", unless user instructed otherwise
if not args.force_update:
    mr_sessions_redcap = mr_sessions_redcap[ ~(mr_sessions_redcap['mr_session_report_complete'] > 1) ]
if args.missing_only:
    mr_sessions_redcap = mr_sessions_redcap[ ~(mr_sessions_redcap['mr_session_report_complete'] > 0) ]

# Filter out all excluded subjects
mr_sessions_redcap = mr_sessions_redcap[ mr_sessions_redcap.index.map( lambda key: False if subject_data['exclude'][key[0]] == 1 else True ) ]

# Filter out all visits where the MR Session report is permanenetly missing 
if mr_sessions_redcap.empty : 
    if args.verbose:
        print "Warning: Nothing to import !" 
    sys.exit()

mr_sessions_redcap = mr_sessions_redcap[ ~(mr_sessions_redcap['mri_missing'] > 0) ]

if args.verbose:
    print "Checking %d REDCap records." % len( mr_sessions_redcap )

# Iterate over all remaining rows
records_uploaded = 0
index = 0
# timer is just run once for dicom conversion if timer log dir is set 
runTimerForImportToPipeline=True
for [key, row] in mr_sessions_redcap.iterrows():
    index +=1 
    subject_label = key[0]
    if not subject_label in subject_project_dict:
        if args.verbose: 
            print index, "Passing on", key , " does not exist in XNAT"
        continue

    if args.verbose: 
        print index, "Processing", key 

    # Need to drop it otherwise upload to redcap fails 
    row = row.drop('mri_missing') 

    visit_date = str(visit_log_redcap['visit_date'][key])
    if visit_date == 'nan':
        if float(row['visit_ignore___yes']) != 1:
            error = 'Missing visit date for subject with visit data.'
            slog.info(key[0], error,
                          visit_id=key[1])
    else:
        redcap_visit_id=subject_label+"-"+visit_date

        this_subject_data = subject_data.ix[key[0]]
        if str( this_subject_data['dob'] ) == 'nan':
            print "Missing birthdate for subject %s" % key[0]
        else:
            # Update XNAT SID and PID for subjects that changed sites
            if subject_label in export_measures_map.iterkeys():
                sub_map = export_measures_map.get(subject_label)
                event = key[1]
                if event == 'baseline_visit_arm_1':
                    id_map = sub_map.get('baseline')
                    sid = id_map.get('subject')
                elif event == '1y_visit_arm_1' and sub_map.get('followup_1y'):
                    id_map = sub_map.get('followup_1y')
                    sid = id_map.get('subject')
                elif event == '2y_visit_arm_1' and sub_map.get('followup_2y'):
                    id_map = sub_map.get('followup_2y')
                    sid = id_map.get('subject')
                elif event == '3y_visit_arm_1' and sub_map.get('followup_3y'):
                    id_map = sub_map.get('followup_3y')
                    sid = id_map.get('subject')
                elif event == '4y_visit_arm_1' and sub_map.get('followup_4y'):
                    id_map = sub_map.get('followup_4y')
                    sid = id_map.get('subject')
                else:
                    id_map = sub_map.get('default')
                    sid = id_map.get('xnat_subject_id')
                pipe_id = id_map.get('subject')
                pid = id_map.get('project')
            else :
                sid, pid = subject_project_dict[subject_label]
                pipe_id = sid

            visit_age = days_between_dates( this_subject_data['dob'], visit_date ) / 365.242
            next_visit_date = get_subject_next_visit_date(key[0], visit_date)
            xnat_data = get_xnat_data(key,
                                      xnat,
                                      sid,
                                      pid,
                                      pipe_id,
                                      visit_date,
                                      this_subject_data['dob'],
                                      next_visit_date,
                                      row['mri_inspection___completed'],
                                      verbose=args.verbose)


            if xnat_data['mri_xnat_eids'] != '':
                # Check whether this MR session also has Stroop data
                (stroop_eid,stroop_resource,stroop_file) = (None, None, None)
                if not args.no_stroop:
                    if args.force_update_stroop or not visit_log_redcap['mri_stroop_complete'][key] > 0:
                        (stroop_eid,stroop_resource,stroop_file) = stroop.check_for_stroop( xnat, xnat_data['mri_xnat_eids'].split( ' ' ), verbose=args.verbose )
                        if stroop_eid != None:
                            stroop.import_stroop_to_redcap( xnat, stroop_eid, stroop_resource, stroop_file, key, verbose=args.verbose, no_upload=args.no_upload, post_to_github=args.post_to_github)

                # Check if pipeline directory given and export imaging series there
                if args.pipeline_root_dir and (this_subject_data['exclude'] != 1):
                    if mrpipeline.export_and_queue( redcap_visit_id, xnat, xnat_data, key, args.pipeline_root_dir, run_pipeline_script=args.run_pipeline_script, stroop=(stroop_eid,stroop_resource,stroop_file), verbose=args.verbose , timerFlag = runTimerForImportToPipeline) :
                        # only run timer once - otherwise collect too much data
                        runTimerForImportToPipeline = False

            if not args.no_upload and (xnat_data['mr_session_report_complete'] > 0 or args.force_update):
                # Make session data into dict for REDCap import
                record = dict( row )
                (record['study_id'], record['redcap_event_name']) = key
                for (key,value) in xnat_data.iteritems():
                    record[key] = value

                # only record the first upload - otherwise creating too many records
                if records_uploaded :
                    timer_label = None
                else :
                    timer_label = "mr_session_report"

                import_response =  session.redcap_import_records( redcap_visit_id, timer_label, [record]) 
                if not import_response : 
                    continue

                if 'count' in import_response.keys():
                    records_uploaded += import_response['count']

                if 'error' in import_response.keys():
                    slog.info(redcap_visit_id, "UPLOAD ERROR: {}".format(import_response['error']), record)

                if 'records' in import_response.keys():
                    for r in import_response['records']:
                        print "\t", r

if args.verbose:
    if not args.no_upload:
        print "Successfully uploaded %d/%d records to REDCap." % ( records_uploaded, len( mr_sessions_redcap ) )
    else:
        print "Suppressed uploading of %d records to REDCap." % ( len( mr_sessions_redcap ) )

slog.takeTimer1("script_time","{'TotalSessions': " + str(len(mr_sessions_redcap)) + ", 'RecordsUploaded': " +  str(records_uploaded) + "}")

#!/usr/bin/env python

##
##  See COPYING file distributed along with the ncanda-data-integration package
##  for the copyright and license terms
##

# Setup command line parser
import os
import re
import sys
import string
import hashlib
import argparse

import pandas

import redcap

parser = argparse.ArgumentParser( description="Import contents of CSV file into longitudinal REDCap project" )
parser.add_argument( "-v", "--verbose", help="Verbose operation", action="store_true")
parser.add_argument( "-f", "--force", help="Force overwriting of existing records.", action="store_true")
parser.add_argument( "--api-key-file", help="File containing API Key for REDCap project", default=os.path.join( os.path.expanduser("~"), '.server_config/redcap-dataentry-token' ) )
parser.add_argument( "--data-access-group", help="REDCap project data access group that the imported data should be assigned to.", default=None )
parser.add_argument( "csvfile", help="Input .csv file.")
args = parser.parse_args()

# Read input file
data = pandas.io.parsers.read_csv( args.csvfile )

# Replace periods in column labels with underscores
data.rename( columns = lambda s: string.lower( s.replace( '.', '_' ) ), inplace=True )

# Bring original "siteid" column back to assign each record to the correct data access group
if not args.data_access_group == None:
    data['redcap_data_access_group'] = args.data_access_group

# First REDCap connection for the Summary project (this is where we put data)
def connect_to_redcap():
    summary_key_file = open(os.path.join( os.path.expanduser("~"), '.server_config/redcap-dataentry-token'), 'r')
    summary_api_key = summary_key_file.read().strip()
    redcap_server = 'https://ncanda.sri.com/redcap/api/'
    rc = redcap.Project(redcap_server, summary_api_key, verify_ssl=False)
    return rc

# If connection to redcap server fail, try multiple times
try:
    project = connect_to_redcap()
except Exception, error:
    sibis.logging(hashlib.sha1('csv2redcap').hexdigest()[0:6],
    "ERROR: Could not connect to redcap!",
    script = 'csv2redcap')
    sys.exit()


# Get "complete" field of existing records so we can protect "Complete" records from overwriting
complete_field = ''
if not args.force:
    for var in data.columns:
        if re.match( '.*_complete$', var ):
            complete_field = var

# Is there a "complete" field? Then get existing records and ditch all records from new data that are "Complete"
existing_data=pandas.DataFrame
if complete_field != '':
    existing_data = project.export_records( fields=[complete_field], format='df', df_kwargs={"index_col":[project.def_field, "redcap_event_name"]} )

# Make list of dicts for REDCap import
record_list = []
for [key,row] in data.iterrows():
    redcap_key = ( row['study_id'], row['redcap_event_name'] )
    if complete_field != '' and redcap_key in existing_data.index.tolist():
        if existing_data[complete_field][redcap_key] == 2:
            continue
    if not args.force:
        record = dict( row.dropna() )
    else:
        record = dict( row.fillna('') )
    record_list.append( record )

# Upload new data to REDCap
import_response = project.import_records( record_list, overwrite='overwrite' )

# If there were any errors, try to print them as well as possible
if 'error' in import_response.keys():
    print "UPLOAD ERROR:", import_response['error']

if 'fields' in import_response.keys():
    for field in import_response['fields']:
        print "\t", field

if 'records' in import_response.keys():
    for record in import_response['records']:
        print "\t", record

# Finally, print upload status if so desired
if args.verbose and 'count' in import_response.keys():
    print "Successfully uploaded %d/%d records to REDCap." % ( import_response['count'], len( data ) )

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import redcap as rc\n",
    "import numpy as np\n",
    "import sibispy\n",
    "from sibispy import sibislogger as slog\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sibispy.Session()\n",
    "if not session.configure():\n",
    "    sys.exit()\n",
    "\n",
    "slog.init_log(None, None, \n",
    "              'QC: Check all harvester-prepared CSVs are uploaded', \n",
    "              'laptop_import_check', None)\n",
    "slog.startTimer1()\n",
    "\n",
    "# Setting specific constants for this run of QC\n",
    "api = session.connect_server('import_laptops', True)\n",
    "primary_key = api.def_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = api.export_metadata(format='df')\n",
    "form_names = meta.form_name.unique().tolist()\n",
    "form_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#form_names_subset = [f for f in form_names if not f.startswith('limesurvey')]\n",
    "form_names_subset = form_names\n",
    "form_names_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from http://pycap.readthedocs.io/en/latest/deep.html#dealing-with-large-exports\n",
    "# and adapted to scope down to forms\n",
    "def chunked_export(project, form, chunk_size=100, verbose=True):\n",
    "    def chunks(l, n):\n",
    "        \"\"\"Yield successive n-sized chunks from list l\"\"\"\n",
    "        for i in xrange(0, len(l), n):\n",
    "            yield l[i:i+n]\n",
    "    record_list = project.export_records(fields=[project.def_field])\n",
    "    records = [r[project.def_field] for r in record_list]\n",
    "    #print \"Total records: %d\" % len(records)\n",
    "    try:\n",
    "        response = None\n",
    "        record_count = 0\n",
    "        for record_chunk in chunks(records, chunk_size):\n",
    "            record_count = record_count + chunk_size\n",
    "            #print record_count\n",
    "            chunked_response = project.export_records(records=record_chunk, \n",
    "                                                      fields=[project.def_field],\n",
    "                                                      forms=[form], \n",
    "                                                      format='df',\n",
    "                                                      df_kwargs={'low_memory': False})\n",
    "            if response is not None:\n",
    "                response = pd.concat([response, chunked_response], axis=0)\n",
    "            else:\n",
    "                response = chunked_response\n",
    "    except rc.RedcapError:\n",
    "        msg = \"Chunked export failed for chunk_size={:d}\".format(chunk_size)\n",
    "        raise ValueError(msg)\n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_form(api, form_name, verbose=True):\n",
    "    if verbose:\n",
    "        print form_name\n",
    "    \n",
    "    # 1. Standard load attempt\n",
    "    # try:\n",
    "    #     print \"Trying standard export\"\n",
    "    #     return api.export_records(fields=[api.def_field],\n",
    "    #                               forms=[form_name],\n",
    "    #                               format='df',\n",
    "    #                               df_kwargs={'low_memory': False})\n",
    "    # except (ValueError, rc.RedcapError, pd.io.common.EmptyDataError):\n",
    "    #     pass\n",
    "    try:\n",
    "        print \"Trying chunked export, 5000 records at a time\"\n",
    "        return chunked_export(api, form_name, 5000)\n",
    "    except (ValueError, rc.RedcapError, pd.io.common.EmptyDataError):\n",
    "        pass\n",
    "    \n",
    "    # 2. Chunked load with chunk size of 1000\n",
    "    try:\n",
    "        print \"Trying chunked export, 1000 records at a time\"\n",
    "        return chunked_export(api, form_name, 1000)\n",
    "    except (ValueError, rc.RedcapError, pd.io.common.EmptyDataError):\n",
    "        pass\n",
    "    \n",
    "    # 2. Chunked load with default chunk size\n",
    "    try:\n",
    "        print \"Trying chunked export, default chunk size (100)\"\n",
    "        return chunked_export(api, form_name, 100)\n",
    "    except (ValueError, rc.RedcapError, pd.io.common.EmptyDataError):\n",
    "        pass\n",
    "    \n",
    "    # 3. Chunked load with tiny chunk\n",
    "    try:\n",
    "        print \"Trying chunked export with tiny chunks (10)\"\n",
    "        return chunked_export(api, form_name, 10)\n",
    "    except (ValueError, rc.RedcapError, pd.io.common.EmptyDataError):\n",
    "        print \"Giving up\"\n",
    "        return None\n",
    "\n",
    "def load_form_with_primary_key(api, form_name, verbose=True):\n",
    "    df = load_form(api, form_name, verbose)\n",
    "    if df is not None:\n",
    "        return df.set_index(api.def_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {form_name: load_form_with_primary_key(api, form_name) for form_name in form_names_subset}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extract emptiness statistic from Import records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_non_nan_rowwise(df, form_name=None, drop_column=None):\n",
    "    \"\"\" A more efficient method of checking non-NaN values \"\"\"\n",
    "    # 1. check complete\n",
    "    if form_name:\n",
    "        complete_field = form_name + '_complete'\n",
    "        if drop_columns:\n",
    "            drop_columns.append(complete_field)\n",
    "        else:\n",
    "            drop_columns = [complete_field]\n",
    "    if drop_columns is None:\n",
    "        drop_columns = []\n",
    "    \n",
    "    # 2. count up NaNs\n",
    "    return df.drop(drop_columns, axis=1).notnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to DF to get all empty records\n",
    "def set_emptiness_flags(row, form_name, drop_columns=None):\n",
    "    # 1. check complete\n",
    "    complete_field = form_name + '_complete'\n",
    "    #is_incomplete = row[complete_field] == 0  # TODO: maybe complete_field not in [1, 2] to catch NaNs?\n",
    "    \n",
    "    # 2. count up NaNs\n",
    "    if drop_columns:\n",
    "        drop_columns.append(complete_field)\n",
    "    else:\n",
    "        drop_columns = [complete_field]\n",
    "    # NOTE: This will only work for a Series\n",
    "    # NOTE: For a full Data Frame, use df.drop(drop_columns, axis=1).notnull().sum(axis=1)\n",
    "    non_nan_count = row.drop(drop_columns).notnull().sum()\n",
    "    \n",
    "    return pd.Series({'completion_status': row[complete_field], 'non_nan_count': non_nan_count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emptiness_df = {form_name: all_data[form_name].apply(lambda x: set_emptiness_flags(x, form_name), axis=1) \n",
    "                for form_name in all_data.keys() \n",
    "                if all_data[form_name] is not None}\n",
    "#all_data['recovery_questionnaire'].apply(lambda x: set_emptiness_flags(x, 'recovery_questionnaire'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for form_name in emptiness_df.keys():\n",
    "    emptiness_df[form_name]['form'] = form_name\n",
    "all_forms_emptiness = pd.concat(emptiness_df.values())\n",
    "all_forms_emptiness.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_to_long = {\n",
    "    # Forms for Arm 1: Standard Protocol  \n",
    "    'dd100': 'delayed_discounting_100',\n",
    "    'dd1000': 'delayed_discounting_1000',\n",
    "\n",
    "    'pasat': 'paced_auditory_serial_addition_test_pasat',\n",
    "    'stroop': 'stroop',\n",
    "    \n",
    "    'ssaga_youth': 'ssaga_youth',\n",
    "    'ssaga_parent': 'ssaga_parent',\n",
    "    'youthreport1': 'youth_report_1',\n",
    "    'youthreport1b': 'youth_report_1b',\n",
    "    'youthreport2': 'youth_report_2',\n",
    "    'parentreport': 'parent_report',\n",
    "    \n",
    "    'mrireport': 'mri_report',\n",
    "    'plus': 'participant_last_use_summary',\n",
    "    \n",
    "    'myy': 'midyear_youth_interview',\n",
    "    \n",
    "    'lssaga1_youth': 'limesurvey_ssaga_part_1_youth',\n",
    "    'lssaga2_youth': 'limesurvey_ssaga_part_2_youth',\n",
    "    'lssaga3_youth': 'limesurvey_ssaga_part_3_youth',\n",
    "    'lssaga4_youth': 'limesurvey_ssaga_part_4_youth',\n",
    "    \n",
    "    'lssaga1_parent': 'limesurvey_ssaga_part_1_parent',\n",
    "    'lssaga2_parent': 'limesurvey_ssaga_part_2_parent',\n",
    "    'lssaga3_parent': 'limesurvey_ssaga_part_3_parent',\n",
    "    'lssaga4_parent': 'limesurvey_ssaga_part_4_parent',\n",
    "\n",
    "    # Forms for Arm 3: Sleep Studies\n",
    "    'sleepeve': 'sleep_study_evening_questionnaire',\n",
    "    'sleeppre': 'sleep_study_presleep_questionnaire',\n",
    "    'sleepmor': 'sleep_study_morning_questionnaire',\n",
    "\n",
    "    # Forms for Recovery project\n",
    "    'recq': 'recovery_questionnaire',\n",
    "    \n",
    "    # Forms for UCSD\n",
    "    'parent': 'ssaga_parent',\n",
    "    'youth': 'ssaga_youth',\n",
    "    'deldisc': 'delayed_discounting'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df = pd.DataFrame(columns=[\"file\", \"path\", \"form\"])\n",
    "records = []\n",
    "record_paths = []\n",
    "for root, subdirs, files in os.walk('/fs/storage/laptops/imported'):\n",
    "    csv_files = [f for f in files if (f.endswith(\".csv\") and not f.endswith(\"-fields.csv\"))]\n",
    "    if csv_files:\n",
    "        folder_df = pd.DataFrame(columns=[\"file\", \"path\", \"form\"])\n",
    "        folder_df['file'] = csv_files\n",
    "        folder_df['path'] = [root + \"/\" + f for f in csv_files]\n",
    "        \n",
    "        root_parts = root.split('/')\n",
    "        current_folder = root_parts[-1]\n",
    "        try:\n",
    "            form = short_to_long[current_folder]\n",
    "            if form not in form_names_subset:\n",
    "                continue\n",
    "            else:\n",
    "                folder_df['form'] = form\n",
    "                files_df = pd.concat([files_df, folder_df])\n",
    "            \n",
    "        except KeyError as e:\n",
    "            continue\n",
    "files_df.set_index(\"path\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRecordIDFromFile(row):\n",
    "    import re\n",
    "    bare_file = re.sub(r\"\\.csv$\", \"\", row[\"file\"])\n",
    "    if row[\"form\"] == \"delayed_discounting\":\n",
    "        bare_file = re.sub(\"-1000?$\", \"\", bare_file)\n",
    "    return bare_file\n",
    "files_df[\"record_id\"] = files_df.apply(getRecordIDFromFile, axis=1)\n",
    "files_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixFormName(row):\n",
    "    import re\n",
    "    if row[\"form\"] == \"delayed_discounting\":\n",
    "        if re.search(r\"-100\\.csv$\", row[\"file\"]):\n",
    "            return \"delayed_discounting_100\"\n",
    "        elif re.search(r\"-1000\\.csv$\", row[\"file\"]):\n",
    "            return \"delayed_discounting_1000\"\n",
    "        else:\n",
    "            return \"delayed_discounting\"\n",
    "    else:\n",
    "        return row[\"form\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df[\"form\"] = files_df.apply(fixFormName, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_forms_emptiness.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_redcap = pd.merge(files_df.reset_index(),\n",
    "                           all_forms_emptiness.reset_index(), \n",
    "                           on=[\"record_id\", \"form\"], \n",
    "                           how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_redcap.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Get results\n",
    "## Files that weren't matched at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_redcap.loc[files_in_redcap.completion_status.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files that were matched but have blank forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_redcap.loc[files_in_redcap['path'].notnull() & (files_in_redcap['non_nan_count'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_file_empty(row):\n",
    "    contents = pd.read_csv(row['path'])\n",
    "    return contents.dropna(axis=\"columns\").shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(files_in_redcap\n",
    " .loc[files_in_redcap['path'].notnull() & \n",
    "      (files_in_redcap['non_nan_count'] == 0)]\n",
    " .apply(check_if_file_empty, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Records that don't match harvester CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_redcap.loc[files_in_redcap['path'].isnull() & (files_in_redcap['non_nan_count'] > 0)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

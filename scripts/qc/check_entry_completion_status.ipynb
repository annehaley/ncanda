{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this script as a standard Jupyter Notebook inside a production container (e.g. `pipeline-back`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import redcap as rc\n",
    "import numpy as np\n",
    "from operator import itemgetter  # for extracting multiple keys from a dict\n",
    "\n",
    "import sibispy\n",
    "from sibispy import sibislogger as slog\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting pandas options for interactive displays\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sibispy.Session()\n",
    "if not session.configure():\n",
    "    sys.exit()\n",
    "\n",
    "slog.init_log(None, None, 'QC_all: Completeness report', 'qc_all', None)\n",
    "slog.startTimer1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up API and specific constants for this run of QC\n",
    "api = session.connect_server('data_entry', True)\n",
    "primary_key = api.def_field\n",
    "\n",
    "qc_event = '3y_visit_arm_1'\n",
    "qc_arm = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forms_for_qc(rc_api, event, arm=1):\n",
    "    forms_events = rc_api.export_fem(format='df')\n",
    "    if \"form\" in forms_events.columns:\n",
    "        form_key = \"form\"\n",
    "    else:\n",
    "        form_key = \"form_name\"\n",
    "    return (forms_events\n",
    "            .query('unique_event_name == @event & arm_num == @arm', engine=\"python\")\n",
    "            .get(form_key).unique().tolist())\n",
    "forms_for_qc = get_forms_for_qc(api, qc_event, qc_arm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it would be useful to:\n",
    "\n",
    "1. create a dictionary with each form as a dictionary key\n",
    "2. iterate through the dictionary and for each key, export records as df and store that as the value in the dictionary\n",
    "3. optionally, merge the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forms_to_skip = ['biological_np', 'biological_mr', 'mr_session_report', 'mri_report']\n",
    "[forms_for_qc.remove(form) for form in forms_to_skip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forms_for_qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_event(rc_api, event, forms=None, arm=1):\n",
    "    if forms is None:\n",
    "        forms = get_forms_for_qc(rc_api, event, arm)\n",
    "    \n",
    "    return dict((form_name, \n",
    "                 rc_api.export_records(format='df',\n",
    "                                       fields=[rc_api.def_field], \n",
    "                                       events=[event],\n",
    "                                       forms=[form_name]).\\\n",
    "                 reset_index(level='redcap_event_name', \n",
    "                             drop=True)) \n",
    "                for form_name in forms)\n",
    "data_all = get_data_for_event(api, qc_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the intermediate product if needed\n",
    "save_to_csv = False\n",
    "if save_to_csv:\n",
    "    [df.to_csv(\"data_20180226/\" + form + '.csv') for form, df in data_all.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_df_from_forms(form_names, data_all):\n",
    "    \"\"\" Return a single pandas.DataFrame merged from data frames stored in a dict.\n",
    "    \n",
    "    `form_names` is assumed to be a list of keys, all of which exist in `data_all`.\n",
    "    \n",
    "    All data frames in the dictionary are assumed to be sharing their index.\"\"\"\n",
    "    selected_forms = itemgetter(*form_names)(data_all)\n",
    "    return reduce(lambda x, y: pd.merge(x, y, left_index=True, right_index=True), selected_forms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata / notes / dates\n",
    "A data frame to later left-join in order to check that there's no documented reason for the flagged concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_meta = {form: data_all[form].filter(regex=r'_date$|_notes?$') for form in forms_for_qc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = reduce(lambda x, y: pd.merge(x, y, left_index=True, right_index=True), \n",
    "                 all_meta.values())\n",
    "notes_df = meta_df.filter(regex=r'_notes?$')\n",
    "dates_df = meta_df.filter(regex=r'_date$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presence check: visit\n",
    "\n",
    "Participants should either have a visit date or should be marked and explained for having missed it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all['visit_date'].query('visit_date != visit_date & visit_ignore___yes != 1', engine=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality / presence check: MRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Do you have a visit date?\n",
    "def has_visit(all_forms_data):\n",
    "    return all_forms_data['visit_date']['visit_date'].notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_missing_scans(all_forms_data):\n",
    "    # 2. Do you have any missing scan dates?\n",
    "    missing_scan_date = all_forms_data['mr_session_report'].\\\n",
    "        filter(regex=r'_date$').isnull().any(axis=1)\n",
    "    # 3. Are you marked as not missing?\n",
    "    not_marked_missing = all_forms_data['mr_session_report']['mri_missing'].isnull()\n",
    "    return missing_scan_date & not_marked_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mr_flags(all_forms_data):\n",
    "    flagged_subset = all_forms_data['mr_session_report'][has_visit(data_all) & \n",
    "                                                         has_missing_scans(data_all)]\n",
    "    return pd.merge(flagged_subset, \n",
    "                    data_all['visit_date'], \n",
    "                    how='left', \n",
    "                    left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mr_flags(data_all).filter(regex=r'^mri_xnat|_date$|missing$|visit_notes|mri_notes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, the follow-up would be to:\n",
    "\n",
    "1. Check the notes of this final subset to see if scan absence is indicated. If not, contact the sites.\n",
    "2. Ask sites to either (1) upload the scans or (2) update `mri_missing` (if _all_ scans are missing) or `mri_notes` (if _some_ scans are missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality check: NPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the general quality check follows this logic:\n",
    "\n",
    "1. Is there a visit date?\n",
    "2. If yes, are the NPs filled out? / How many NPs in each form are filled out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_keys = filter(lambda x: x.startswith('np'), data_all.keys())\n",
    "np_keys = ['visit_date'] + np_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data = itemgetter(*np_keys)(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging on indices which are guaranteed to be shared\n",
    "np_data_all = reduce(lambda x, y: pd.merge(x, y, left_index=True, right_index=True), np_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With most forms, we might be interested in the number of non-answers. After all, there's a lot of fields that patients fill out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#{form: df.columns.tolist() for form, df in data_all.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...so we'll re-shape them from wide to long and count up the absences.\n",
    "\n",
    "## WRAT4\n",
    "\n",
    "Trying the approach out on WRAT4, we do the following:\n",
    "\n",
    "1. check that the record is not missing (should also check presence of visit date - or maybe subjects without visit date should have been purged at an earlier stage?)\n",
    "2. check that the record is completed (=2)\n",
    "3. stack the substantive fields and flag subjects with fewer than max responses\n",
    "\n",
    "Step 3 ensures that we don't flag forms nobody has answered. It does rely on the intuition that if someone has answered a field, everyone should. It would fail for casees where many people have the same *number* of non-answers, but each in different fields. It certainly fails in more complex forms where some a field's necessity is conditional on previous responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^((?!missing|complete).)*$ is negative lookahead matching everything except the string 'missing'\n",
    "np_iter = data_all['np_wrat4_word_reading_and_math_computation']\\\n",
    "    .query('np_wrat4_missing != 1 & np_wrat4_word_reading_and_math_computation_complete == 2', engine=\"python\")\\\n",
    "    .filter(regex=r'^((?!missing|complete).)*$')\\\n",
    "    .stack(dropna=False)\n",
    "    #set_index(['np_wrat4_missing', 'np_wrat4_missing_why', 'np_wrat4_missing_why_other'], append=True).\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np_iter.count(level=0)\n",
    "incomplete = counts < counts.max()\n",
    "# note: I can't use this to index the original frame because this Series is shorter; \n",
    "#       I can't use .index because all the participants are still in it, regardless\n",
    "#       of truth or falsity of `incomplete`. There should be a more elegant way to do this.\n",
    "#\n",
    "# Maybe a variation on dfd.loc[dfd.index[[0, 2]]]?\n",
    "counts_incomplete = counts[incomplete] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_incomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np_iter.loc[counts_incomplete.index.tolist()].isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wider subset of NP reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_forms = forms_for_qc[9:15] # could also filter with a regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~~Approach 1: Merge all items and evaluate them together~~\n",
    "This could work but might not have the level of granularity that is useful to a human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np_data = merge_df_from_forms(['visit_date'] + np_data, data_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Evaluate each form separately, bring together the results\n",
    "\n",
    "Using the same drill as for WRAT4. Of course, one issue is that some forms really contain multiple subforms that are not  easily separable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items_containing(needle, haystack):\n",
    "    return [elt for elt in haystack if needle in elt]\n",
    "def get_items_matching_regex(regex, haystack):\n",
    "    return filter(lambda x: re.search(regex, x), haystack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{form: get_items_matching_regex(r\"complete$|missing$\", df.columns) for form, df in data_all.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the \"multiple\" subforms e.g. in form `clinical`, which has multiple completion variables (but no missingness variables - which is a separate problem).\n",
    "\n",
    "As for why they're not easily separable, consider that the naming convention isn't stable: in `biological_mr`, the completion variable is `biological_mr_complete`, but the missingness variable is `bio_mr_complete`.\n",
    "\n",
    "Since there's only 35 forms, and only a bunch don't follow rules, creating custom rules for each in order to be able to do the broader QC check on all is not impossible, merely inelegant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_potential_omissions_marked_complete(df):\n",
    "    # some forms don't have it, some forms have it for multiple subsections\n",
    "    cols_missing = get_items_matching_regex(r'missing$', df.columns)\n",
    "    # all forms have it, some forms have it for multiple subsections\n",
    "    cols_complete = get_items_matching_regex(\"complete$\", df.columns)\n",
    "    \n",
    "    if len(cols_complete) > 1: # TODO: In the future, don't give up\n",
    "        return \"Too many complete fields\"\n",
    "    \n",
    "    # Constructing the query\n",
    "    # \n",
    "    # We want to flag a participant for whom:\n",
    "    # 1. one or more subsections were *not* marked missing\n",
    "    # 2. one or more subsections of the record were marked complete\n",
    "    # \n",
    "    # The issue making this difficult is that there isn't necessarily a\n",
    "    # one-to-one matching between the presence of missingness cols and \n",
    "    # the presence of completeness cols.\n",
    "    #\n",
    "    # In general, though, if at least one section is not marked missing,\n",
    "    # and if at least one section is marked complete, the subject is game.\n",
    "    #\n",
    "    # (Future direction: plug-in lambdas for each form?)\n",
    "    query_present_complete = \"%s == 2\" % cols_complete[0]\n",
    "    if len(cols_missing) == 1:\n",
    "        query_present = \"%s != 1\" % cols_missing[0]\n",
    "        query_present_complete = query_present_complete + \" and \" + query_present\n",
    "    \n",
    "    df_relevant = df.query(query_present_complete, engine=\"python\")\n",
    "    if len(df_relevant) == 0:\n",
    "        return \"No participants filled out the form\"\n",
    "    \n",
    "    # Now, remove all complete / missing columns and stack\n",
    "    df_relevant = df_relevant.filter(regex=r'^((?!missing|complete).)*$').\\\n",
    "        stack(dropna=False)\n",
    "    counts = df_relevant.count(level=0)\n",
    "    incomplete = counts < counts.max()\n",
    "    counts_incomplete = counts[incomplete]\n",
    "    \n",
    "    return counts_incomplete.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# map(flag_incomplete_forms, np_data)\n",
    "np_flags = {form: flag_potential_omissions_marked_complete(data_all[form]) for form in np_forms}\n",
    "np_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_flags = {form: flag_potential_omissions_marked_complete(data_all[form]) for form in forms_for_qc}\n",
    "all_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checker(flags_dict, data_dict):\n",
    "    def check_form(form_name):\n",
    "        if form_name not in flags_dict or form_name not in data_dict:\n",
    "            return None\n",
    "        else:\n",
    "            return pd.merge(notes_df, data_dict[form_name].loc[flags_dict[form_name]],\n",
    "                            how='right', left_index=True, right_index=True)\n",
    "    return check_form\n",
    "check_form = checker(all_flags, data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_form('brief')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_form('stroop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_form('np_wrat4_word_reading_and_math_computation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_form('np_reyosterrieth_complex_figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_form('np_reyosterrieth_complex_figure_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_form('np_modified_greglygraybiel_test_of_ataxia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_form('parent_report')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_form('participant_last_use_summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flagged_data = {form: data_all[form].loc[flags] for form, flags in all_flags.values() if isinstance(flags, list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incomplete forms?\n",
    "\n",
    "After talking to Devin, Ryan and Vanessa, I realized that the scope of this check was limited.\n",
    "\n",
    "The previous check could best be characterized as \"flagging records that were mistakenly labeled 'complete' when they were, in fact, missing information.\" This lets off the hook the records that were not marked missing and should have been.\n",
    "\n",
    "But the interplay of `*_missing` and `*_complete` has slightly more complex rules / usage:\n",
    "\n",
    "1. If a form is marked missing, it should always be marked \"complete\". It might not, in practice, be marked that, but it should be.\n",
    "2. If a form isn't marked missing, it might be marked either:\n",
    "    a. **incomplete** (\"missing\" in some data dictionaries, which is confusing): the data exists, but it hasn't been entered yet.\n",
    "    b. **unverified**: the data has been entered, but should be re-checked,\n",
    "    c. **complete**: the data as entered is currently assumed to be the complete record for that form.\n",
    "   \n",
    "So, essentially, there are five states a form can be in, and their failure modes are different:\n",
    "\n",
    "1. **Marked permanently missing**: should not have any data in it, and completion mark doesn't matter (but it _should_ be marked \"complete\"). Flag if:\n",
    "    * Has data, or\n",
    "    * Is not marked complete. (In theory, the script could auto-mark it complete, as there is no other resolution.)\n",
    "2. **Not missing but incomplete**: site should enter data. **This is the default status of a form.** Flag if:\n",
    "    * The deadline for data collection has passed. (Data should either be filled in and form should be marked complete, or the form should be marked permanently missing and complete.)\n",
    "3. **Not missing but unverified**: site should do the second part of double entry. Flag if:\n",
    "    * The deadline for data collection has passed.\n",
    "    * _Maybe?_ The data has been entered into the form more than n days ago.\n",
    "4. **Not missing and complete**: site is done with the form (but might have overlooked something). Flag if:\n",
    "    * Participant is missing some responses that other participants have. (This is the original logic of this PR.)\n",
    "5. **Not marked anything**: In theory, this shouldn't happen at all, so it should always be flagged.\n",
    "\n",
    "The previous check doesn't bother with (1); it only addresses (4).\n",
    "\n",
    "## Checking for visit date?\n",
    "This is where it might make sense to check if the participant has _any_ data for the year.\n",
    "\n",
    "Then again, maybe not? Global collection status may not matter to a form.\n",
    "\n",
    "## Checking for \"should this visit be ignored?\"\n",
    "In general, yes - e.g. exclusions at baseline will be marked that every year, and there's no way to tell just one form at a time.\n",
    "\n",
    "(In the future, we might want to use specific exceptions for exclusion at baseline, but that doesn't change the case here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issue with checkboxes\n",
    "In some forms - biological NP, biological MR, maybe others? - checkboxes are used to solicit response. The default value that will be exported, **even if the form was not filled out**, is 0. Depending on whether the participant actually saw and filled out the form, the value `0` should be interpreted as either `0` (if seen) or `NaN` (if not seen).\n",
    "\n",
    "This can be inferred from the presence of a date on _some_ forms, but not all. Here's a list of checkboxes that can be a problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forms_with_checkboxes = {\n",
    "    'np_grooved_pegboard': ['np_gpeg_exclusion___dh', 'np_gpeg_exclusion___ndh'],\n",
    "    'np_reyosterreith_complex_figure_files': ['np_reyo_ndh___yes'],\n",
    "    'np_wais4_coding': ['np_wais4_rawscore_diff___correct'],\n",
    "    'cnp_summary': [\n",
    "             'cnp_instruments___testsessions',\n",
    "             'cnp_instruments___cpf',\n",
    "             'cnp_instruments___cpfd',\n",
    "             'cnp_instruments___cpw',\n",
    "             'cnp_instruments___cpwd',\n",
    "             'cnp_instruments___medf36',\n",
    "             'cnp_instruments___er40d',\n",
    "             'cnp_instruments___mpract',\n",
    "             'cnp_instruments___pcet',\n",
    "             'cnp_instruments___pmat24a',\n",
    "             'cnp_instruments___pvoc',\n",
    "             'cnp_instruments___pvrt',\n",
    "             'cnp_instruments___sfnb2',\n",
    "             'cnp_instruments___shortvolt',\n",
    "             'cnp_instruments___spcptnl',\n",
    "             'cnp_instruments___svdelay',\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_and_complete_column_names(df):\n",
    "    # some forms don't have it, some forms have it for multiple subsections\n",
    "    cols_missing = get_items_matching_regex(r'missing$', df.columns)\n",
    "    # all forms have it, some forms have it for multiple subsections\n",
    "    cols_complete = get_items_matching_regex(\"complete$\", df.columns)\n",
    "    \n",
    "    return cols_missing, cols_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news is, there don't appear to be any data lost behind records labeled \"missing\". Of course, if REDCap wipes the data when the form is marked missing, that would be impossible.\n",
    "\n",
    "False positives are due to checkboxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def flag_missing_with_data(df):\n",
    "    # 1. is it marked missing\n",
    "    # 2. does it have any fields that are filled out?\n",
    "    cols_missing, cols_complete = get_missing_and_complete_column_names(df)\n",
    "    \n",
    "    if len(cols_missing) == 1:\n",
    "        query_missing = \"%s == 1\" % cols_missing[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    df_relevant = df.query(query_missing, engine=\"python\")\n",
    "    if len(df_relevant) == 0:\n",
    "        return None\n",
    "\n",
    "    # Now, remove all complete / missing columns and stack\n",
    "    df_relevant = df_relevant.filter(regex=r'^((?!missing|complete).)*$').\\\n",
    "        stack(dropna=True).count(level=0)\n",
    "    return df_relevant[df_relevant > 0].index.tolist()\n",
    "\n",
    "{form: flag_missing_with_data(data_all[form]) for form in forms_for_qc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def flag_missing_not_marked_complete(df):\n",
    "    # 1. is it flagged missing\n",
    "    # 2. is it not flagged complete\n",
    "    cols_missing, cols_complete = get_missing_and_complete_column_names(df)\n",
    "    query_not_complete = \" or \".join([\"%s == 2\" % col for col in cols_complete])\n",
    "    \n",
    "    if len(cols_missing) == 1:\n",
    "        query_missing = \"%s == 1\" % cols_missing[0]\n",
    "        query_not_complete = \"(\" + query_not_complete + \") and \" + query_missing\n",
    "    else:\n",
    "        return None\n",
    " \n",
    "    df_relevant = df.query(query_not_complete, engine=\"python\")\n",
    "    if len(df_relevant) == 0:\n",
    "        return None\n",
    "    return df_relevant.index.tolist()\n",
    "\n",
    "    # Now, remove all complete / missing columns and stack\n",
    "    df_relevant = df_relevant.filter(regex=r'^((?!missing|complete).)*$').\\\n",
    "        stack(dropna=False).count(level=0)\n",
    "    return df_relevant[df_relevant == 0].index.tolist()\n",
    "{form: flag_missing_not_marked_complete(data_all[form]) for form in forms_for_qc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where checkboxes are causing a lot of false _negatives_. Since the checkbox reports in as 0, it appears as though there's data.\n",
    "\n",
    "This is also the most common status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def flag_incomplete_without_data(df):\n",
    "    # Equivalent of grey circle in REDCap.\n",
    "    #\n",
    "    # 1. is it marked incomplete\n",
    "    # 2. is it not marked missing\n",
    "    # 3. does it have any data\n",
    "    \n",
    "    # By default, forms are marked to receive more data, so this will return almost everything.\n",
    "    # If they haven't received data by collection deadline, should be marked missing+complete\n",
    "    cols_missing, cols_complete = get_missing_and_complete_column_names(df)\n",
    "    query_incomplete = \" or \".join([\"%s == 0\" % col for col in cols_complete])\n",
    "    \n",
    "    if len(cols_missing) == 1:\n",
    "        query_present = \"%s != 1\" % cols_missing[0]\n",
    "        query_incomplete = \"(\" + query_incomplete + \") and \" + query_present\n",
    " \n",
    "    df_relevant = df.query(query_incomplete, engine=\"python\")\n",
    "    if len(df_relevant) == 0:\n",
    "        return None\n",
    "\n",
    "    # Now, remove all complete / missing columns and stack\n",
    "    df_relevant = df_relevant.filter(regex=r'^((?!missing|complete).)*$').\\\n",
    "        stack(dropna=False).count(level=0)\n",
    "    return df_relevant[df_relevant == 0].index.tolist()\n",
    "\n",
    "{form: flag_incomplete_without_data(data_all[form]) for form in forms_for_qc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The checkboxes are, again, causing a lot of false positives here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def flag_incomplete_with_data(df):\n",
    "    # This is the equivalent of a red circle in REDCap.\n",
    "    #\n",
    "    # 1. is it marked incomplete\n",
    "    # 2. is it not marked missing\n",
    "    # 3. does it have any data\n",
    "    #\n",
    "    # Follow-up: Site should work to fill in data, or mark record as complete\n",
    "    #  (or delete data if there was a mix-up)\n",
    "    \n",
    "    cols_missing, cols_complete = get_missing_and_complete_column_names(df)\n",
    "    query_incomplete = \" or \".join([\"%s == 0\" % col for col in cols_complete])\n",
    "    \n",
    "    if len(cols_missing) == 1:\n",
    "        query_present = \"%s != 1\" % cols_missing[0]\n",
    "        query_incomplete = \"(\" + query_incomplete + \") and \" + query_present\n",
    " \n",
    "    df_relevant = df.query(query_incomplete, engine=\"python\")\n",
    "    if len(df_relevant) == 0:\n",
    "        return None\n",
    "\n",
    "    # Now, remove all complete / missing columns and stack\n",
    "    df_relevant = df_relevant.filter(regex=r'^((?!missing|complete).)*$').\\\n",
    "        stack(dropna=True).count(level=0)\n",
    "    return df_relevant[df_relevant > 0].index.tolist()\n",
    "\n",
    "{form: flag_incomplete_with_data(data_all[form]) for form in forms_for_qc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def flag_unverified_forms(df):\n",
    "    # These forms are marked to receive more data. \n",
    "    # It doesn't matter whether they're marked missing or not; they should be verified.\n",
    "    \n",
    "    cols_missing, cols_complete = get_missing_and_complete_column_names(df)\n",
    "    query_unverified = \" or \".join([\"%s == 1\" % col for col in cols_complete])\n",
    "        \n",
    "    df_relevant = df.query(query_unverified, engine=\"python\")\n",
    "    if len(df_relevant) == 0:\n",
    "        return None\n",
    "    return df_relevant.index.tolist()\n",
    "unverified_flags = {form: flag_unverified_forms(data_all[form]) for form in forms_for_qc}\n",
    "unverified_flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide flags up by site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_flags_for_site(flags, site):  # Site is assumed to be a single character\n",
    "    import re\n",
    "    if flags:\n",
    "        return [flag for flag in flags if re.match(r'^' + site, flag)]\n",
    "    else:\n",
    "        return None\n",
    "def divide_flags_by_site(flags_by_form, sites = ['A', 'B', 'C', 'D', 'E']):\n",
    "    by_site = dict.fromkeys(sites)\n",
    "    for site in sites:\n",
    "        by_site[site] = {form_name: extract_flags_for_site(form, site) for (form_name, form) in flags_by_form.items()}\n",
    "    return by_site\n",
    "unverified_by_site = divide_flags_by_site(unverified_flags)\n",
    "unverified_by_site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_mapping = {'baseline': 70,\n",
    "                 '6-month': 71,\n",
    "                 '1y': 72,\n",
    "                 '18-month': 73,\n",
    "                 '2y': 74,\n",
    "                 '30-month': 75,\n",
    "                 '3y': 76,\n",
    "                 '42-month': 77,\n",
    "                 '4y': 78}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_urls_for_ids_by_form(flags_by_form, event_id=76):\n",
    "    root_url = 'https://ncanda.sri.com/redcap/redcap_v6.10.5/DataEntry/index.php'\n",
    "    entry_link_schema = root_url + '?pid=20&id=%s&event_id=%d&page=%s'\n",
    "    links = {}\n",
    "    for form_name, form_flags in flags_by_form.iteritems():\n",
    "        if form_flags:\n",
    "            links[form_name] = {study_id: entry_link_schema % (study_id, event_id, form_name) for study_id in form_flags}\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unverified_urls_by_site = {site: make_urls_for_ids_by_form(site_unverified) \n",
    "                           for site, site_unverified \n",
    "                           in unverified_by_site.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for site, flags_by_form in unverified_urls_by_site.iteritems():\n",
    "    print('# %s\\n' % site)\n",
    "    for form_name, flags_in_form in flags_by_form.iteritems():\n",
    "        print('## %s\\n' % form_name)\n",
    "        for study_id, url in flags_in_form.iteritems():\n",
    "            print('* [%s](%s)' % (study_id, url))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
